{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ..utils.get_iou import get_max_iou, get_iou, get_overlap\n",
    "import numpy as np\n",
    "\n",
    "from ..utils.get_models_and_set import get_model, evaluate_models\n",
    "from ..utils.get_models_and_set import get_train_data_latex, get_train_data_word, get_train_data_publaynet\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib notebook\n",
    "import glob\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "from IPython.display import Image\n",
    "import time\n",
    "import shutil\n",
    "from os import path\n",
    "\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "# importing required libraries\n",
    "import pandas as pd\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "# import some common detectron2 utilities\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.structures import BoxMode\n",
    "from detectron2.utils.visualizer import ColorMode\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import DatasetCatalog\n",
    "from detectron2.structures import BoxMode\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.evaluation import COCOEvaluator\n",
    "from detectron2.engine import DefaultTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = \"/data/rali5/Tmp/yockelle/TableBank/TableBank/Detection/images\"\n",
    "thres = 0.9\n",
    "\n",
    "class CocoTrainer(DefaultTrainer):\n",
    "\n",
    "  @classmethod\n",
    "  def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
    "\n",
    "    if output_folder is None:\n",
    "        os.makedirs(\"coco_eval\", exist_ok=True)\n",
    "        output_folder = \"coco_eval\"\n",
    "\n",
    "    return COCOEvaluator(dataset_name, cfg, False, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File count: 278581\n"
     ]
    }
   ],
   "source": [
    "# folder path\n",
    "#dir_path = \"/data/rali5/Tmp/yockelle/TableBank/TableBank/Detection/dilated_images\"\n",
    "#count = 0\n",
    "# Iterate directory\n",
    "#for path in os.listdir(dir_path):\n",
    "#    # check if current path is a file\n",
    "#    if os.path.isfile(os.path.join(dir_path, path)):\n",
    "#        count += 1\n",
    "#print('File count:', count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data_word_latex = get_train_data_word() + get_train_data_latex()\n",
    "#count = 0\n",
    "#for img in train_data_word_latex:\n",
    "#    image_name = img[\"file_name\"]\n",
    "#    if not(os.path.exists(image_name)):\n",
    "#        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data_with_smuge():\n",
    "    train_data_word_latex = get_train_data_word() + get_train_data_latex()\n",
    "    train_data_word_latex_smudge = []\n",
    "    for img in train_data_word_latex:\n",
    "        image_name = img[\"file_name\"]\n",
    "        image_name = image_name.replace(\"images\", \"transformed_images\")\n",
    "        final_name = \"Smudge_\" + image_name.split(\"/\")[-1]\n",
    "        smuge_name = \"/\".join(image_name.split(\"/\")[0:-1]) + \"/\" + final_name\n",
    "        if os.path.exists(smuge_name):\n",
    "            img[\"file_name\"] = smuge_name\n",
    "            train_data_word_latex_smudge.append(img)\n",
    "\n",
    "    return train_data_word_latex + train_data_word_latex_smudge\n",
    "\n",
    "\n",
    "def get_train_word_with_smudge():\n",
    "    train_data_word = get_train_data_word()\n",
    "    train_data_word_smudge = []\n",
    "    for img in train_data_word:\n",
    "        image_name = img[\"file_name\"]\n",
    "        image_name = image_name.replace(\"images\", \"smudge_images\")\n",
    "        final_name = \"Smudge_\" + image_name.split(\"/\")[-1]\n",
    "        smuge_name = \"/\".join(image_name.split(\"/\")[0:-1]) + \"/\" + final_name\n",
    "        if os.path.exists(smuge_name):\n",
    "            img[\"file_name\"] = smuge_name\n",
    "            train_data_word_smudge.append(img)\n",
    "    return train_data_word_smudge + train_data_word\n",
    "\n",
    "def get_train_word_with_dilation():\n",
    "    train_data_word = get_train_data_word()\n",
    "    train_data_word_dilatation = []\n",
    "    for img in train_data_word:\n",
    "        image_name = img[\"file_name\"]\n",
    "        image_name = image_name.replace(\"images\", \"dilated_images\")\n",
    "        final_name = \"Dilation_\" + image_name.split(\"/\")[-1]\n",
    "        smuge_name = \"/\".join(image_name.split(\"/\")[0:-1]) + \"/\" + final_name\n",
    "        if os.path.exists(smuge_name):\n",
    "            img[\"file_name\"] = smuge_name\n",
    "            train_data_word_dilatation.append(img)\n",
    "    return train_data_word_dilatation + train_data_word\n",
    "\n",
    "def get_train_word_with_smuge_and_dilation():\n",
    "    train_data_word = get_train_data_word()\n",
    "    train_data_word_smudge = []\n",
    "    for img in train_data_word:\n",
    "        image_name = img[\"file_name\"]\n",
    "        image_name = image_name.replace(\"images\", \"smudge_images\")\n",
    "        final_name = \"Smudge_\" + image_name.split(\"/\")[-1]\n",
    "        smuge_name = \"/\".join(image_name.split(\"/\")[0:-1]) + \"/\" + final_name\n",
    "        if os.path.exists(smuge_name):\n",
    "            img[\"file_name\"] = smuge_name\n",
    "            train_data_word_smudge.append(img)\n",
    "\n",
    "    train_data_word = get_train_data_word()\n",
    "    train_data_word_dilatation = []\n",
    "    for img in train_data_word:\n",
    "        image_name = img[\"file_name\"]\n",
    "        image_name = image_name.replace(\"images\", \"dilated_images\")\n",
    "        final_name = \"Dilation_\" + image_name.split(\"/\")[-1]\n",
    "        dilation_name = \"/\".join(image_name.split(\"/\")[0:-1]) + \"/\" + final_name\n",
    "        if os.path.exists(dilation_name):\n",
    "            img[\"file_name\"] = dilation_name\n",
    "            train_data_word_dilatation.append(img)\n",
    "\n",
    "    return train_data_word + train_data_word_smudge + train_data_word_dilatation\n",
    "\n",
    "\n",
    "def get_train_latex_with_smudge():\n",
    "    train_data_latex = get_train_data_latex()\n",
    "    train_data_word_smudge = []\n",
    "    for img in train_data_latex:\n",
    "        image_name = img[\"file_name\"]\n",
    "        image_name = image_name.replace(\"images\", \"smudge_images\")\n",
    "        final_name = \"Smudge_\" + image_name.split(\"/\")[-1]\n",
    "        smuge_name = \"/\".join(image_name.split(\"/\")[0:-1]) + \"/\" + final_name\n",
    "        if os.path.exists(smuge_name):\n",
    "            img[\"file_name\"] = smuge_name\n",
    "            train_data_word_smudge.append(img)\n",
    "    return train_data_word_smudge + train_data_latex\n",
    "\n",
    "def get_train_latex_with_dilation():\n",
    "    train_data_latex = get_train_data_latex()\n",
    "    train_data_word_dilatation = []\n",
    "    for img in train_data_latex:\n",
    "        image_name = img[\"file_name\"]\n",
    "        image_name = image_name.replace(\"images\", \"dilated_images\")\n",
    "        final_name = \"Dilation_\" + image_name.split(\"/\")[-1]\n",
    "        smuge_name = \"/\".join(image_name.split(\"/\")[0:-1]) + \"/\" + final_name\n",
    "        if os.path.exists(smuge_name):\n",
    "            img[\"file_name\"] = smuge_name\n",
    "            train_data_word_dilatation.append(img)\n",
    "    return train_data_word_dilatation + train_data_latex\n",
    "\n",
    "def get_train_latex_with_smuge_and_dilation():\n",
    "    train_data_word = get_train_data_latex()\n",
    "    train_data_word_smudge = []\n",
    "    for img in train_data_word:\n",
    "        image_name = img[\"file_name\"]\n",
    "        image_name = image_name.replace(\"images\", \"smudge_images\")\n",
    "        final_name = \"Smudge_\" + image_name.split(\"/\")[-1]\n",
    "        smuge_name = \"/\".join(image_name.split(\"/\")[0:-1]) + \"/\" + final_name\n",
    "        if os.path.exists(smuge_name):\n",
    "            img[\"file_name\"] = smuge_name\n",
    "            train_data_word_smudge.append(img)\n",
    "\n",
    "    train_data_word = get_train_data_latex()\n",
    "    train_data_word_dilatation = []\n",
    "    for img in train_data_word:\n",
    "        image_name = img[\"file_name\"]\n",
    "        image_name = image_name.replace(\"images\", \"dilated_images\")\n",
    "        final_name = \"Dilation_\" + image_name.split(\"/\")[-1]\n",
    "        dilation_name = \"/\".join(image_name.split(\"/\")[0:-1]) + \"/\" + final_name\n",
    "        if os.path.exists(dilation_name):\n",
    "            img[\"file_name\"] = dilation_name\n",
    "            train_data_word_dilatation.append(img)\n",
    "\n",
    "    return train_data_word + train_data_word_smudge + train_data_word_dilatation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[02/01 19:40:21 d2.engine.defaults]: \u001b[0mModel:\n",
      "GeneralizedRCNN(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelMaxPool()\n",
      "    (bottom_up): ResNet(\n",
      "      (stem): BasicStem(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (res2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (4): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (5): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res5): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): StandardROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc_relu1): ReLU()\n",
      "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (fc_relu2): ReLU()\n",
      "    )\n",
      "    (box_predictor): FastRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m[02/01 19:42:01 d2.data.build]: \u001b[0mDistribution of instances among all 1 categories:\n",
      "\u001b[36m|  category  | #instances   |\n",
      "|:----------:|:-------------|\n",
      "|   table    | 474862       |\n",
      "|            |              |\u001b[0m\n",
      "\u001b[32m[02/01 19:42:01 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
      "\u001b[32m[02/01 19:42:01 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
      "\u001b[32m[02/01 19:42:01 d2.data.common]: \u001b[0mSerializing 374398 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[02/01 19:42:03 d2.data.common]: \u001b[0mSerialized dataset takes 164.93 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (2, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (2,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (4, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (4,) in the model! You might want to double check if this is expected.\n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_predictor.cls_score.{bias, weight}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[02/01 19:42:06 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n",
      "\u001b[32m[02/01 19:42:14 d2.utils.events]: \u001b[0m eta: 0:18:38  iter: 19  total_loss: 0.6296  loss_cls: 0.3665  loss_box_reg: 0.2456  loss_rpn_cls: 0.01503  loss_rpn_loc: 0.01638  time: 0.3816  data_time: 0.0296  lr: 0.00024976  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:42:22 d2.utils.events]: \u001b[0m eta: 0:18:57  iter: 39  total_loss: 0.4635  loss_cls: 0.1856  loss_box_reg: 0.2312  loss_rpn_cls: 0.01314  loss_rpn_loc: 0.01186  time: 0.3862  data_time: 0.0113  lr: 0.00049951  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:42:30 d2.utils.events]: \u001b[0m eta: 0:18:46  iter: 59  total_loss: 0.4499  loss_cls: 0.1534  loss_box_reg: 0.2861  loss_rpn_cls: 0.004129  loss_rpn_loc: 0.01322  time: 0.3838  data_time: 0.0071  lr: 0.00074926  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:42:38 d2.utils.events]: \u001b[0m eta: 0:18:43  iter: 79  total_loss: 0.3532  loss_cls: 0.1075  loss_box_reg: 0.2432  loss_rpn_cls: 0.004834  loss_rpn_loc: 0.01234  time: 0.3849  data_time: 0.0082  lr: 0.00099901  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:42:45 d2.utils.events]: \u001b[0m eta: 0:18:38  iter: 99  total_loss: 0.3063  loss_cls: 0.09702  loss_box_reg: 0.1836  loss_rpn_cls: 0.006592  loss_rpn_loc: 0.0141  time: 0.3854  data_time: 0.0089  lr: 0.0012488  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:42:53 d2.utils.events]: \u001b[0m eta: 0:18:36  iter: 119  total_loss: 0.1973  loss_cls: 0.07058  loss_box_reg: 0.1085  loss_rpn_cls: 0.002751  loss_rpn_loc: 0.008867  time: 0.3856  data_time: 0.0086  lr: 0.0014985  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:43:01 d2.utils.events]: \u001b[0m eta: 0:18:23  iter: 139  total_loss: 0.1438  loss_cls: 0.04791  loss_box_reg: 0.08215  loss_rpn_cls: 0.0007545  loss_rpn_loc: 0.005067  time: 0.3848  data_time: 0.0079  lr: 0.0017483  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:43:09 d2.utils.events]: \u001b[0m eta: 0:18:26  iter: 159  total_loss: 0.1426  loss_cls: 0.0458  loss_box_reg: 0.07767  loss_rpn_cls: 0.003272  loss_rpn_loc: 0.006497  time: 0.3864  data_time: 0.0088  lr: 0.001998  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:43:17 d2.utils.events]: \u001b[0m eta: 0:18:17  iter: 179  total_loss: 0.1185  loss_cls: 0.0394  loss_box_reg: 0.06653  loss_rpn_cls: 0.00599  loss_rpn_loc: 0.008917  time: 0.3864  data_time: 0.0080  lr: 0.0022478  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:43:24 d2.utils.events]: \u001b[0m eta: 0:18:09  iter: 199  total_loss: 0.1245  loss_cls: 0.04054  loss_box_reg: 0.07116  loss_rpn_cls: 0.001228  loss_rpn_loc: 0.006986  time: 0.3864  data_time: 0.0084  lr: 0.0024975  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:43:32 d2.utils.events]: \u001b[0m eta: 0:18:01  iter: 219  total_loss: 0.1165  loss_cls: 0.04391  loss_box_reg: 0.06634  loss_rpn_cls: 0.0009436  loss_rpn_loc: 0.007873  time: 0.3867  data_time: 0.0088  lr: 0.0027473  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:43:40 d2.utils.events]: \u001b[0m eta: 0:17:58  iter: 239  total_loss: 0.1131  loss_cls: 0.03368  loss_box_reg: 0.06447  loss_rpn_cls: 0.0008933  loss_rpn_loc: 0.007282  time: 0.3879  data_time: 0.0085  lr: 0.002997  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:43:48 d2.utils.events]: \u001b[0m eta: 0:17:51  iter: 259  total_loss: 0.1058  loss_cls: 0.03863  loss_box_reg: 0.05615  loss_rpn_cls: 0.001961  loss_rpn_loc: 0.009643  time: 0.3885  data_time: 0.0086  lr: 0.0032468  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:43:56 d2.utils.events]: \u001b[0m eta: 0:17:44  iter: 279  total_loss: 0.1136  loss_cls: 0.03731  loss_box_reg: 0.05328  loss_rpn_cls: 0.001849  loss_rpn_loc: 0.006388  time: 0.3885  data_time: 0.0086  lr: 0.0034965  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:44:04 d2.utils.events]: \u001b[0m eta: 0:17:36  iter: 299  total_loss: 0.114  loss_cls: 0.04239  loss_box_reg: 0.06299  loss_rpn_cls: 0.004699  loss_rpn_loc: 0.009911  time: 0.3883  data_time: 0.0083  lr: 0.0037463  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:44:11 d2.utils.events]: \u001b[0m eta: 0:17:27  iter: 319  total_loss: 0.1021  loss_cls: 0.03442  loss_box_reg: 0.05687  loss_rpn_cls: 0.001947  loss_rpn_loc: 0.006858  time: 0.3882  data_time: 0.0081  lr: 0.003996  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:44:19 d2.utils.events]: \u001b[0m eta: 0:17:20  iter: 339  total_loss: 0.1105  loss_cls: 0.035  loss_box_reg: 0.06441  loss_rpn_cls: 0.0007368  loss_rpn_loc: 0.006929  time: 0.3884  data_time: 0.0082  lr: 0.0042458  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:44:27 d2.utils.events]: \u001b[0m eta: 0:17:16  iter: 359  total_loss: 0.129  loss_cls: 0.03743  loss_box_reg: 0.07131  loss_rpn_cls: 0.001055  loss_rpn_loc: 0.00872  time: 0.3888  data_time: 0.0087  lr: 0.0044955  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:44:35 d2.utils.events]: \u001b[0m eta: 0:17:06  iter: 379  total_loss: 0.1193  loss_cls: 0.03706  loss_box_reg: 0.06894  loss_rpn_cls: 0.0008946  loss_rpn_loc: 0.007336  time: 0.3887  data_time: 0.0083  lr: 0.0047453  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:44:43 d2.utils.events]: \u001b[0m eta: 0:16:56  iter: 399  total_loss: 0.1146  loss_cls: 0.03951  loss_box_reg: 0.06809  loss_rpn_cls: 0.0008041  loss_rpn_loc: 0.006506  time: 0.3888  data_time: 0.0086  lr: 0.004995  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:44:50 d2.utils.events]: \u001b[0m eta: 0:16:47  iter: 419  total_loss: 0.1148  loss_cls: 0.03144  loss_box_reg: 0.06289  loss_rpn_cls: 0.002513  loss_rpn_loc: 0.00926  time: 0.3882  data_time: 0.0085  lr: 0.0052448  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:44:58 d2.utils.events]: \u001b[0m eta: 0:16:39  iter: 439  total_loss: 0.1085  loss_cls: 0.03293  loss_box_reg: 0.06503  loss_rpn_cls: 0.0007964  loss_rpn_loc: 0.005899  time: 0.3881  data_time: 0.0091  lr: 0.0054945  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:45:06 d2.utils.events]: \u001b[0m eta: 0:16:31  iter: 459  total_loss: 0.09314  loss_cls: 0.03488  loss_box_reg: 0.05054  loss_rpn_cls: 0.002174  loss_rpn_loc: 0.006474  time: 0.3883  data_time: 0.0091  lr: 0.0057443  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:45:14 d2.utils.events]: \u001b[0m eta: 0:16:23  iter: 479  total_loss: 0.09102  loss_cls: 0.0223  loss_box_reg: 0.05738  loss_rpn_cls: 0.0006635  loss_rpn_loc: 0.00725  time: 0.3880  data_time: 0.0086  lr: 0.005994  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:45:21 d2.utils.events]: \u001b[0m eta: 0:16:16  iter: 499  total_loss: 0.1143  loss_cls: 0.03588  loss_box_reg: 0.05896  loss_rpn_cls: 0.0007563  loss_rpn_loc: 0.01013  time: 0.3878  data_time: 0.0089  lr: 0.0062438  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:45:29 d2.utils.events]: \u001b[0m eta: 0:16:08  iter: 519  total_loss: 0.1254  loss_cls: 0.03282  loss_box_reg: 0.07776  loss_rpn_cls: 0.0009105  loss_rpn_loc: 0.007061  time: 0.3874  data_time: 0.0083  lr: 0.0064935  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:45:37 d2.utils.events]: \u001b[0m eta: 0:16:00  iter: 539  total_loss: 0.1077  loss_cls: 0.03276  loss_box_reg: 0.0534  loss_rpn_cls: 0.003138  loss_rpn_loc: 0.01116  time: 0.3874  data_time: 0.0081  lr: 0.0067433  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:45:45 d2.utils.events]: \u001b[0m eta: 0:15:52  iter: 559  total_loss: 0.08756  loss_cls: 0.02626  loss_box_reg: 0.04722  loss_rpn_cls: 0.00193  loss_rpn_loc: 0.006499  time: 0.3876  data_time: 0.0089  lr: 0.006993  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:45:52 d2.utils.events]: \u001b[0m eta: 0:15:44  iter: 579  total_loss: 0.0834  loss_cls: 0.02597  loss_box_reg: 0.04473  loss_rpn_cls: 0.001815  loss_rpn_loc: 0.006631  time: 0.3876  data_time: 0.0081  lr: 0.0072428  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:46:00 d2.utils.events]: \u001b[0m eta: 0:15:37  iter: 599  total_loss: 0.1017  loss_cls: 0.02645  loss_box_reg: 0.05645  loss_rpn_cls: 0.002345  loss_rpn_loc: 0.01008  time: 0.3878  data_time: 0.0086  lr: 0.0074925  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:46:08 d2.utils.events]: \u001b[0m eta: 0:15:29  iter: 619  total_loss: 0.1019  loss_cls: 0.02596  loss_box_reg: 0.06047  loss_rpn_cls: 0.001776  loss_rpn_loc: 0.009308  time: 0.3879  data_time: 0.0086  lr: 0.0077423  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:46:16 d2.utils.events]: \u001b[0m eta: 0:15:21  iter: 639  total_loss: 0.09916  loss_cls: 0.02367  loss_box_reg: 0.06437  loss_rpn_cls: 0.0007213  loss_rpn_loc: 0.006971  time: 0.3879  data_time: 0.0086  lr: 0.007992  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:46:24 d2.utils.events]: \u001b[0m eta: 0:15:13  iter: 659  total_loss: 0.1024  loss_cls: 0.02962  loss_box_reg: 0.06336  loss_rpn_cls: 0.0008803  loss_rpn_loc: 0.00632  time: 0.3878  data_time: 0.0083  lr: 0.0082418  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:46:31 d2.utils.events]: \u001b[0m eta: 0:15:04  iter: 679  total_loss: 0.1069  loss_cls: 0.02946  loss_box_reg: 0.05425  loss_rpn_cls: 0.002013  loss_rpn_loc: 0.008952  time: 0.3876  data_time: 0.0076  lr: 0.0084915  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:46:39 d2.utils.events]: \u001b[0m eta: 0:14:57  iter: 699  total_loss: 0.07931  loss_cls: 0.02314  loss_box_reg: 0.04944  loss_rpn_cls: 0.001214  loss_rpn_loc: 0.007106  time: 0.3877  data_time: 0.0083  lr: 0.0087413  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:46:47 d2.utils.events]: \u001b[0m eta: 0:14:49  iter: 719  total_loss: 0.1029  loss_cls: 0.02826  loss_box_reg: 0.05765  loss_rpn_cls: 0.00421  loss_rpn_loc: 0.008902  time: 0.3876  data_time: 0.0082  lr: 0.008991  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:46:54 d2.utils.events]: \u001b[0m eta: 0:14:41  iter: 739  total_loss: 0.08451  loss_cls: 0.02683  loss_box_reg: 0.04463  loss_rpn_cls: 0.002625  loss_rpn_loc: 0.007459  time: 0.3874  data_time: 0.0075  lr: 0.0092408  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:47:02 d2.utils.events]: \u001b[0m eta: 0:14:33  iter: 759  total_loss: 0.08339  loss_cls: 0.02573  loss_box_reg: 0.04193  loss_rpn_cls: 0.002462  loss_rpn_loc: 0.00931  time: 0.3874  data_time: 0.0085  lr: 0.0094905  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:47:10 d2.utils.events]: \u001b[0m eta: 0:14:24  iter: 779  total_loss: 0.08631  loss_cls: 0.02233  loss_box_reg: 0.05154  loss_rpn_cls: 0.001214  loss_rpn_loc: 0.008367  time: 0.3871  data_time: 0.0088  lr: 0.0097403  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:47:17 d2.utils.events]: \u001b[0m eta: 0:14:16  iter: 799  total_loss: 0.08378  loss_cls: 0.02568  loss_box_reg: 0.05233  loss_rpn_cls: 0.001549  loss_rpn_loc: 0.006905  time: 0.3869  data_time: 0.0074  lr: 0.00999  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:47:25 d2.utils.events]: \u001b[0m eta: 0:14:08  iter: 819  total_loss: 0.1018  loss_cls: 0.03323  loss_box_reg: 0.05191  loss_rpn_cls: 0.002735  loss_rpn_loc: 0.007567  time: 0.3869  data_time: 0.0085  lr: 0.01024  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:47:33 d2.utils.events]: \u001b[0m eta: 0:14:01  iter: 839  total_loss: 0.1221  loss_cls: 0.03942  loss_box_reg: 0.07171  loss_rpn_cls: 0.002801  loss_rpn_loc: 0.008938  time: 0.3869  data_time: 0.0079  lr: 0.01049  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:47:41 d2.utils.events]: \u001b[0m eta: 0:13:53  iter: 859  total_loss: 0.1119  loss_cls: 0.0389  loss_box_reg: 0.06115  loss_rpn_cls: 0.002744  loss_rpn_loc: 0.009861  time: 0.3869  data_time: 0.0088  lr: 0.010739  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:47:48 d2.utils.events]: \u001b[0m eta: 0:13:44  iter: 879  total_loss: 0.09903  loss_cls: 0.02881  loss_box_reg: 0.05681  loss_rpn_cls: 0.0009528  loss_rpn_loc: 0.007403  time: 0.3867  data_time: 0.0084  lr: 0.010989  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:47:56 d2.utils.events]: \u001b[0m eta: 0:13:36  iter: 899  total_loss: 0.1035  loss_cls: 0.03172  loss_box_reg: 0.05399  loss_rpn_cls: 0.001702  loss_rpn_loc: 0.00532  time: 0.3866  data_time: 0.0084  lr: 0.011239  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:48:04 d2.utils.events]: \u001b[0m eta: 0:13:29  iter: 919  total_loss: 0.09303  loss_cls: 0.02465  loss_box_reg: 0.0531  loss_rpn_cls: 0.001585  loss_rpn_loc: 0.005286  time: 0.3867  data_time: 0.0088  lr: 0.011489  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:48:11 d2.utils.events]: \u001b[0m eta: 0:13:21  iter: 939  total_loss: 0.09366  loss_cls: 0.02074  loss_box_reg: 0.05945  loss_rpn_cls: 0.00094  loss_rpn_loc: 0.008491  time: 0.3867  data_time: 0.0081  lr: 0.011738  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:48:19 d2.utils.events]: \u001b[0m eta: 0:13:13  iter: 959  total_loss: 0.1032  loss_cls: 0.02648  loss_box_reg: 0.06026  loss_rpn_cls: 0.001311  loss_rpn_loc: 0.007141  time: 0.3866  data_time: 0.0076  lr: 0.011988  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:48:27 d2.utils.events]: \u001b[0m eta: 0:13:06  iter: 979  total_loss: 0.1012  loss_cls: 0.02231  loss_box_reg: 0.05908  loss_rpn_cls: 0.001706  loss_rpn_loc: 0.006963  time: 0.3867  data_time: 0.0076  lr: 0.012238  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:48:34 d2.utils.events]: \u001b[0m eta: 0:12:58  iter: 999  total_loss: 0.1252  loss_cls: 0.03791  loss_box_reg: 0.06827  loss_rpn_cls: 0.001419  loss_rpn_loc: 0.00574  time: 0.3865  data_time: 0.0074  lr: 0.012488  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:48:42 d2.utils.events]: \u001b[0m eta: 0:12:50  iter: 1019  total_loss: 0.09324  loss_cls: 0.02553  loss_box_reg: 0.05465  loss_rpn_cls: 0.002263  loss_rpn_loc: 0.009458  time: 0.3864  data_time: 0.0083  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:48:50 d2.utils.events]: \u001b[0m eta: 0:12:42  iter: 1039  total_loss: 0.1477  loss_cls: 0.03011  loss_box_reg: 0.07568  loss_rpn_cls: 0.004851  loss_rpn_loc: 0.009539  time: 0.3866  data_time: 0.0082  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:48:58 d2.utils.events]: \u001b[0m eta: 0:12:35  iter: 1059  total_loss: 0.1113  loss_cls: 0.02752  loss_box_reg: 0.06936  loss_rpn_cls: 0.002632  loss_rpn_loc: 0.007301  time: 0.3868  data_time: 0.0079  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:49:06 d2.utils.events]: \u001b[0m eta: 0:12:27  iter: 1079  total_loss: 0.1049  loss_cls: 0.02984  loss_box_reg: 0.06278  loss_rpn_cls: 0.002814  loss_rpn_loc: 0.007305  time: 0.3867  data_time: 0.0088  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:49:14 d2.utils.events]: \u001b[0m eta: 0:12:20  iter: 1099  total_loss: 0.09849  loss_cls: 0.02696  loss_box_reg: 0.05853  loss_rpn_cls: 0.001245  loss_rpn_loc: 0.01002  time: 0.3869  data_time: 0.0081  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:49:21 d2.utils.events]: \u001b[0m eta: 0:12:12  iter: 1119  total_loss: 0.09442  loss_cls: 0.01901  loss_box_reg: 0.04976  loss_rpn_cls: 0.003317  loss_rpn_loc: 0.01169  time: 0.3869  data_time: 0.0076  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:49:29 d2.utils.events]: \u001b[0m eta: 0:12:04  iter: 1139  total_loss: 0.07995  loss_cls: 0.02051  loss_box_reg: 0.05151  loss_rpn_cls: 0.00177  loss_rpn_loc: 0.004455  time: 0.3866  data_time: 0.0066  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:49:37 d2.utils.events]: \u001b[0m eta: 0:11:56  iter: 1159  total_loss: 0.08195  loss_cls: 0.0194  loss_box_reg: 0.05592  loss_rpn_cls: 0.001559  loss_rpn_loc: 0.004687  time: 0.3867  data_time: 0.0079  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:49:45 d2.utils.events]: \u001b[0m eta: 0:11:48  iter: 1179  total_loss: 0.09082  loss_cls: 0.02125  loss_box_reg: 0.0556  loss_rpn_cls: 0.0009909  loss_rpn_loc: 0.006254  time: 0.3865  data_time: 0.0072  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:49:52 d2.utils.events]: \u001b[0m eta: 0:11:40  iter: 1199  total_loss: 0.1003  loss_cls: 0.02394  loss_box_reg: 0.0492  loss_rpn_cls: 0.001674  loss_rpn_loc: 0.007505  time: 0.3865  data_time: 0.0083  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:50:00 d2.utils.events]: \u001b[0m eta: 0:11:33  iter: 1219  total_loss: 0.09827  loss_cls: 0.02691  loss_box_reg: 0.05715  loss_rpn_cls: 0.001217  loss_rpn_loc: 0.006358  time: 0.3865  data_time: 0.0084  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:50:08 d2.utils.events]: \u001b[0m eta: 0:11:24  iter: 1239  total_loss: 0.09687  loss_cls: 0.02612  loss_box_reg: 0.05136  loss_rpn_cls: 0.001667  loss_rpn_loc: 0.00681  time: 0.3865  data_time: 0.0073  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:50:16 d2.utils.events]: \u001b[0m eta: 0:11:16  iter: 1259  total_loss: 0.07686  loss_cls: 0.02213  loss_box_reg: 0.04789  loss_rpn_cls: 0.001103  loss_rpn_loc: 0.00583  time: 0.3866  data_time: 0.0088  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:50:23 d2.utils.events]: \u001b[0m eta: 0:11:08  iter: 1279  total_loss: 0.1109  loss_cls: 0.02581  loss_box_reg: 0.05638  loss_rpn_cls: 0.0064  loss_rpn_loc: 0.005964  time: 0.3864  data_time: 0.0077  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:50:31 d2.utils.events]: \u001b[0m eta: 0:11:00  iter: 1299  total_loss: 0.08386  loss_cls: 0.01902  loss_box_reg: 0.05318  loss_rpn_cls: 0.001589  loss_rpn_loc: 0.006887  time: 0.3864  data_time: 0.0086  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:50:39 d2.utils.events]: \u001b[0m eta: 0:10:52  iter: 1319  total_loss: 0.0899  loss_cls: 0.02245  loss_box_reg: 0.05157  loss_rpn_cls: 0.003677  loss_rpn_loc: 0.007617  time: 0.3863  data_time: 0.0079  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:50:46 d2.utils.events]: \u001b[0m eta: 0:10:44  iter: 1339  total_loss: 0.09272  loss_cls: 0.01882  loss_box_reg: 0.04384  loss_rpn_cls: 0.008443  loss_rpn_loc: 0.009547  time: 0.3863  data_time: 0.0082  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:50:54 d2.utils.events]: \u001b[0m eta: 0:10:36  iter: 1359  total_loss: 0.1062  loss_cls: 0.03266  loss_box_reg: 0.05408  loss_rpn_cls: 0.002639  loss_rpn_loc: 0.007951  time: 0.3864  data_time: 0.0084  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:51:02 d2.utils.events]: \u001b[0m eta: 0:10:29  iter: 1379  total_loss: 0.08342  loss_cls: 0.02086  loss_box_reg: 0.04987  loss_rpn_cls: 0.001445  loss_rpn_loc: 0.008204  time: 0.3865  data_time: 0.0085  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:51:10 d2.utils.events]: \u001b[0m eta: 0:10:21  iter: 1399  total_loss: 0.08161  loss_cls: 0.02188  loss_box_reg: 0.04946  loss_rpn_cls: 0.00156  loss_rpn_loc: 0.009099  time: 0.3864  data_time: 0.0082  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:51:17 d2.utils.events]: \u001b[0m eta: 0:10:14  iter: 1419  total_loss: 0.08308  loss_cls: 0.02224  loss_box_reg: 0.05054  loss_rpn_cls: 0.001171  loss_rpn_loc: 0.006631  time: 0.3864  data_time: 0.0082  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:51:25 d2.utils.events]: \u001b[0m eta: 0:10:06  iter: 1439  total_loss: 0.086  loss_cls: 0.01951  loss_box_reg: 0.04853  loss_rpn_cls: 0.00103  loss_rpn_loc: 0.00859  time: 0.3865  data_time: 0.0085  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:51:33 d2.utils.events]: \u001b[0m eta: 0:09:58  iter: 1459  total_loss: 0.09932  loss_cls: 0.02931  loss_box_reg: 0.05111  loss_rpn_cls: 0.001228  loss_rpn_loc: 0.005017  time: 0.3865  data_time: 0.0083  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:51:41 d2.utils.events]: \u001b[0m eta: 0:09:50  iter: 1479  total_loss: 0.08654  loss_cls: 0.02787  loss_box_reg: 0.04487  loss_rpn_cls: 0.001202  loss_rpn_loc: 0.005871  time: 0.3864  data_time: 0.0076  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:51:49 d2.utils.events]: \u001b[0m eta: 0:09:43  iter: 1499  total_loss: 0.07522  loss_cls: 0.01803  loss_box_reg: 0.0436  loss_rpn_cls: 0.0009303  loss_rpn_loc: 0.005003  time: 0.3865  data_time: 0.0089  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:51:56 d2.utils.events]: \u001b[0m eta: 0:09:35  iter: 1519  total_loss: 0.07611  loss_cls: 0.02207  loss_box_reg: 0.04519  loss_rpn_cls: 0.001148  loss_rpn_loc: 0.004422  time: 0.3865  data_time: 0.0066  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:52:04 d2.utils.events]: \u001b[0m eta: 0:09:28  iter: 1539  total_loss: 0.1064  loss_cls: 0.03961  loss_box_reg: 0.05832  loss_rpn_cls: 0.002694  loss_rpn_loc: 0.01518  time: 0.3867  data_time: 0.0085  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:52:12 d2.utils.events]: \u001b[0m eta: 0:09:20  iter: 1559  total_loss: 0.09383  loss_cls: 0.02496  loss_box_reg: 0.05028  loss_rpn_cls: 0.001626  loss_rpn_loc: 0.009597  time: 0.3866  data_time: 0.0085  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:52:20 d2.utils.events]: \u001b[0m eta: 0:09:12  iter: 1579  total_loss: 0.1098  loss_cls: 0.03631  loss_box_reg: 0.0491  loss_rpn_cls: 0.001827  loss_rpn_loc: 0.0124  time: 0.3865  data_time: 0.0076  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:52:27 d2.utils.events]: \u001b[0m eta: 0:09:04  iter: 1599  total_loss: 0.08852  loss_cls: 0.02734  loss_box_reg: 0.04991  loss_rpn_cls: 0.001053  loss_rpn_loc: 0.005768  time: 0.3864  data_time: 0.0081  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:52:35 d2.utils.events]: \u001b[0m eta: 0:08:55  iter: 1619  total_loss: 0.08699  loss_cls: 0.02101  loss_box_reg: 0.05725  loss_rpn_cls: 0.001724  loss_rpn_loc: 0.007903  time: 0.3863  data_time: 0.0068  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:52:42 d2.utils.events]: \u001b[0m eta: 0:08:48  iter: 1639  total_loss: 0.07542  loss_cls: 0.01911  loss_box_reg: 0.04469  loss_rpn_cls: 0.001604  loss_rpn_loc: 0.005995  time: 0.3862  data_time: 0.0085  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:52:50 d2.utils.events]: \u001b[0m eta: 0:08:40  iter: 1659  total_loss: 0.0909  loss_cls: 0.01926  loss_box_reg: 0.0394  loss_rpn_cls: 0.001969  loss_rpn_loc: 0.007241  time: 0.3861  data_time: 0.0082  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:52:58 d2.utils.events]: \u001b[0m eta: 0:08:32  iter: 1679  total_loss: 0.06596  loss_cls: 0.02085  loss_box_reg: 0.0396  loss_rpn_cls: 0.001422  loss_rpn_loc: 0.007996  time: 0.3861  data_time: 0.0084  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:53:05 d2.utils.events]: \u001b[0m eta: 0:08:24  iter: 1699  total_loss: 0.0839  loss_cls: 0.01977  loss_box_reg: 0.05627  loss_rpn_cls: 0.0009446  loss_rpn_loc: 0.005649  time: 0.3861  data_time: 0.0085  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:53:13 d2.utils.events]: \u001b[0m eta: 0:08:16  iter: 1719  total_loss: 0.07319  loss_cls: 0.02064  loss_box_reg: 0.04353  loss_rpn_cls: 0.0006413  loss_rpn_loc: 0.004623  time: 0.3861  data_time: 0.0081  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:53:21 d2.utils.events]: \u001b[0m eta: 0:08:08  iter: 1739  total_loss: 0.06678  loss_cls: 0.01633  loss_box_reg: 0.04157  loss_rpn_cls: 0.001803  loss_rpn_loc: 0.0059  time: 0.3860  data_time: 0.0088  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:53:28 d2.utils.events]: \u001b[0m eta: 0:08:00  iter: 1759  total_loss: 0.07952  loss_cls: 0.01909  loss_box_reg: 0.05133  loss_rpn_cls: 0.0009085  loss_rpn_loc: 0.005811  time: 0.3860  data_time: 0.0085  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:53:36 d2.utils.events]: \u001b[0m eta: 0:07:52  iter: 1779  total_loss: 0.09112  loss_cls: 0.02537  loss_box_reg: 0.0505  loss_rpn_cls: 0.002588  loss_rpn_loc: 0.007715  time: 0.3860  data_time: 0.0076  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:53:44 d2.utils.events]: \u001b[0m eta: 0:07:45  iter: 1799  total_loss: 0.08825  loss_cls: 0.02416  loss_box_reg: 0.04962  loss_rpn_cls: 0.001865  loss_rpn_loc: 0.008396  time: 0.3859  data_time: 0.0066  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:53:52 d2.utils.events]: \u001b[0m eta: 0:07:37  iter: 1819  total_loss: 0.08602  loss_cls: 0.02615  loss_box_reg: 0.04803  loss_rpn_cls: 0.001341  loss_rpn_loc: 0.003664  time: 0.3860  data_time: 0.0085  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:54:00 d2.utils.events]: \u001b[0m eta: 0:07:30  iter: 1839  total_loss: 0.06885  loss_cls: 0.02252  loss_box_reg: 0.03968  loss_rpn_cls: 0.002129  loss_rpn_loc: 0.006013  time: 0.3860  data_time: 0.0082  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:54:07 d2.utils.events]: \u001b[0m eta: 0:07:22  iter: 1859  total_loss: 0.06953  loss_cls: 0.01845  loss_box_reg: 0.03975  loss_rpn_cls: 0.001592  loss_rpn_loc: 0.006178  time: 0.3861  data_time: 0.0085  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:54:15 d2.utils.events]: \u001b[0m eta: 0:07:14  iter: 1879  total_loss: 0.07175  loss_cls: 0.02005  loss_box_reg: 0.04549  loss_rpn_cls: 0.001646  loss_rpn_loc: 0.00535  time: 0.3860  data_time: 0.0083  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:54:23 d2.utils.events]: \u001b[0m eta: 0:07:07  iter: 1899  total_loss: 0.07401  loss_cls: 0.02272  loss_box_reg: 0.0413  loss_rpn_cls: 0.002043  loss_rpn_loc: 0.005736  time: 0.3860  data_time: 0.0077  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:54:30 d2.utils.events]: \u001b[0m eta: 0:06:59  iter: 1919  total_loss: 0.089  loss_cls: 0.02714  loss_box_reg: 0.04628  loss_rpn_cls: 0.003521  loss_rpn_loc: 0.009196  time: 0.3859  data_time: 0.0073  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:54:38 d2.utils.events]: \u001b[0m eta: 0:06:51  iter: 1939  total_loss: 0.08666  loss_cls: 0.0241  loss_box_reg: 0.05522  loss_rpn_cls: 0.00146  loss_rpn_loc: 0.007519  time: 0.3859  data_time: 0.0081  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:54:46 d2.utils.events]: \u001b[0m eta: 0:06:43  iter: 1959  total_loss: 0.07348  loss_cls: 0.0197  loss_box_reg: 0.05094  loss_rpn_cls: 0.0008163  loss_rpn_loc: 0.004024  time: 0.3859  data_time: 0.0081  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:54:54 d2.utils.events]: \u001b[0m eta: 0:06:35  iter: 1979  total_loss: 0.08132  loss_cls: 0.02164  loss_box_reg: 0.04352  loss_rpn_cls: 0.001405  loss_rpn_loc: 0.005297  time: 0.3860  data_time: 0.0088  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:55:01 d2.utils.events]: \u001b[0m eta: 0:06:28  iter: 1999  total_loss: 0.09765  loss_cls: 0.02533  loss_box_reg: 0.057  loss_rpn_cls: 0.002135  loss_rpn_loc: 0.007901  time: 0.3860  data_time: 0.0085  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:55:09 d2.utils.events]: \u001b[0m eta: 0:06:20  iter: 2019  total_loss: 0.0942  loss_cls: 0.02146  loss_box_reg: 0.0562  loss_rpn_cls: 0.001232  loss_rpn_loc: 0.008384  time: 0.3860  data_time: 0.0083  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:55:17 d2.utils.events]: \u001b[0m eta: 0:06:12  iter: 2039  total_loss: 0.083  loss_cls: 0.02133  loss_box_reg: 0.05156  loss_rpn_cls: 0.001505  loss_rpn_loc: 0.006592  time: 0.3859  data_time: 0.0074  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:55:24 d2.utils.events]: \u001b[0m eta: 0:06:04  iter: 2059  total_loss: 0.1047  loss_cls: 0.02835  loss_box_reg: 0.0627  loss_rpn_cls: 0.000992  loss_rpn_loc: 0.007007  time: 0.3859  data_time: 0.0081  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:55:32 d2.utils.events]: \u001b[0m eta: 0:05:56  iter: 2079  total_loss: 0.09341  loss_cls: 0.03014  loss_box_reg: 0.04858  loss_rpn_cls: 0.00204  loss_rpn_loc: 0.008488  time: 0.3859  data_time: 0.0078  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:55:40 d2.utils.events]: \u001b[0m eta: 0:05:48  iter: 2099  total_loss: 0.07826  loss_cls: 0.02507  loss_box_reg: 0.0471  loss_rpn_cls: 0.0008409  loss_rpn_loc: 0.004222  time: 0.3859  data_time: 0.0071  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:55:48 d2.utils.events]: \u001b[0m eta: 0:05:40  iter: 2119  total_loss: 0.09389  loss_cls: 0.02475  loss_box_reg: 0.04849  loss_rpn_cls: 0.0009684  loss_rpn_loc: 0.007369  time: 0.3860  data_time: 0.0086  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:55:56 d2.utils.events]: \u001b[0m eta: 0:05:33  iter: 2139  total_loss: 0.07674  loss_cls: 0.02093  loss_box_reg: 0.05428  loss_rpn_cls: 0.001367  loss_rpn_loc: 0.007302  time: 0.3860  data_time: 0.0080  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:56:03 d2.utils.events]: \u001b[0m eta: 0:05:25  iter: 2159  total_loss: 0.1006  loss_cls: 0.02829  loss_box_reg: 0.06311  loss_rpn_cls: 0.0012  loss_rpn_loc: 0.009066  time: 0.3860  data_time: 0.0085  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:56:11 d2.utils.events]: \u001b[0m eta: 0:05:18  iter: 2179  total_loss: 0.09299  loss_cls: 0.02457  loss_box_reg: 0.05069  loss_rpn_cls: 0.006075  loss_rpn_loc: 0.00703  time: 0.3861  data_time: 0.0085  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:56:19 d2.utils.events]: \u001b[0m eta: 0:05:10  iter: 2199  total_loss: 0.07459  loss_cls: 0.01829  loss_box_reg: 0.04741  loss_rpn_cls: 0.0008712  loss_rpn_loc: 0.00631  time: 0.3861  data_time: 0.0078  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:56:27 d2.utils.events]: \u001b[0m eta: 0:05:02  iter: 2219  total_loss: 0.07552  loss_cls: 0.01939  loss_box_reg: 0.04005  loss_rpn_cls: 0.001141  loss_rpn_loc: 0.006518  time: 0.3861  data_time: 0.0074  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:56:34 d2.utils.events]: \u001b[0m eta: 0:04:54  iter: 2239  total_loss: 0.06383  loss_cls: 0.01597  loss_box_reg: 0.04124  loss_rpn_cls: 0.0009074  loss_rpn_loc: 0.00414  time: 0.3859  data_time: 0.0069  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:56:42 d2.utils.events]: \u001b[0m eta: 0:04:46  iter: 2259  total_loss: 0.06008  loss_cls: 0.01567  loss_box_reg: 0.03967  loss_rpn_cls: 0.0007232  loss_rpn_loc: 0.004402  time: 0.3860  data_time: 0.0086  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:56:50 d2.utils.events]: \u001b[0m eta: 0:04:39  iter: 2279  total_loss: 0.08369  loss_cls: 0.02335  loss_box_reg: 0.04796  loss_rpn_cls: 0.002128  loss_rpn_loc: 0.006453  time: 0.3861  data_time: 0.0078  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:56:58 d2.utils.events]: \u001b[0m eta: 0:04:31  iter: 2299  total_loss: 0.08745  loss_cls: 0.02231  loss_box_reg: 0.05179  loss_rpn_cls: 0.001814  loss_rpn_loc: 0.007268  time: 0.3860  data_time: 0.0078  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:57:05 d2.utils.events]: \u001b[0m eta: 0:04:24  iter: 2319  total_loss: 0.1122  loss_cls: 0.03228  loss_box_reg: 0.05786  loss_rpn_cls: 0.00301  loss_rpn_loc: 0.01097  time: 0.3860  data_time: 0.0080  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:57:13 d2.utils.events]: \u001b[0m eta: 0:04:16  iter: 2339  total_loss: 0.07678  loss_cls: 0.0256  loss_box_reg: 0.04144  loss_rpn_cls: 0.002562  loss_rpn_loc: 0.008335  time: 0.3860  data_time: 0.0081  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:57:21 d2.utils.events]: \u001b[0m eta: 0:04:08  iter: 2359  total_loss: 0.06449  loss_cls: 0.015  loss_box_reg: 0.04041  loss_rpn_cls: 0.0018  loss_rpn_loc: 0.005445  time: 0.3860  data_time: 0.0083  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:57:28 d2.utils.events]: \u001b[0m eta: 0:04:00  iter: 2379  total_loss: 0.07887  loss_cls: 0.02207  loss_box_reg: 0.04641  loss_rpn_cls: 0.001294  loss_rpn_loc: 0.005283  time: 0.3860  data_time: 0.0082  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:57:36 d2.utils.events]: \u001b[0m eta: 0:03:52  iter: 2399  total_loss: 0.07151  loss_cls: 0.0198  loss_box_reg: 0.04613  loss_rpn_cls: 0.00147  loss_rpn_loc: 0.005183  time: 0.3860  data_time: 0.0080  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:57:44 d2.utils.events]: \u001b[0m eta: 0:03:45  iter: 2419  total_loss: 0.06843  loss_cls: 0.01654  loss_box_reg: 0.04123  loss_rpn_cls: 0.001188  loss_rpn_loc: 0.0067  time: 0.3860  data_time: 0.0085  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:57:52 d2.utils.events]: \u001b[0m eta: 0:03:37  iter: 2439  total_loss: 0.07185  loss_cls: 0.01491  loss_box_reg: 0.03849  loss_rpn_cls: 0.005391  loss_rpn_loc: 0.005194  time: 0.3860  data_time: 0.0084  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:58:00 d2.utils.events]: \u001b[0m eta: 0:03:29  iter: 2459  total_loss: 0.07348  loss_cls: 0.01637  loss_box_reg: 0.04566  loss_rpn_cls: 0.00191  loss_rpn_loc: 0.006161  time: 0.3860  data_time: 0.0087  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:58:07 d2.utils.events]: \u001b[0m eta: 0:03:21  iter: 2479  total_loss: 0.08492  loss_cls: 0.02063  loss_box_reg: 0.05134  loss_rpn_cls: 0.001153  loss_rpn_loc: 0.005811  time: 0.3860  data_time: 0.0076  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:58:15 d2.utils.events]: \u001b[0m eta: 0:03:14  iter: 2499  total_loss: 0.06196  loss_cls: 0.02087  loss_box_reg: 0.03815  loss_rpn_cls: 0.0009065  loss_rpn_loc: 0.005946  time: 0.3861  data_time: 0.0081  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:58:23 d2.utils.events]: \u001b[0m eta: 0:03:06  iter: 2519  total_loss: 0.07893  loss_cls: 0.02288  loss_box_reg: 0.04883  loss_rpn_cls: 0.001739  loss_rpn_loc: 0.007083  time: 0.3861  data_time: 0.0079  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:58:31 d2.utils.events]: \u001b[0m eta: 0:02:58  iter: 2539  total_loss: 0.0674  loss_cls: 0.02084  loss_box_reg: 0.04049  loss_rpn_cls: 0.001035  loss_rpn_loc: 0.005553  time: 0.3861  data_time: 0.0070  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:58:39 d2.utils.events]: \u001b[0m eta: 0:02:50  iter: 2559  total_loss: 0.06709  loss_cls: 0.01613  loss_box_reg: 0.042  loss_rpn_cls: 0.001518  loss_rpn_loc: 0.004692  time: 0.3862  data_time: 0.0082  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:58:46 d2.utils.events]: \u001b[0m eta: 0:02:43  iter: 2579  total_loss: 0.07315  loss_cls: 0.01802  loss_box_reg: 0.04566  loss_rpn_cls: 0.001287  loss_rpn_loc: 0.005336  time: 0.3862  data_time: 0.0081  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:58:54 d2.utils.events]: \u001b[0m eta: 0:02:35  iter: 2599  total_loss: 0.0757  loss_cls: 0.01742  loss_box_reg: 0.04731  loss_rpn_cls: 0.0008033  loss_rpn_loc: 0.004664  time: 0.3863  data_time: 0.0083  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:59:02 d2.utils.events]: \u001b[0m eta: 0:02:27  iter: 2619  total_loss: 0.07043  loss_cls: 0.01734  loss_box_reg: 0.04187  loss_rpn_cls: 0.0006621  loss_rpn_loc: 0.003736  time: 0.3863  data_time: 0.0085  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:59:10 d2.utils.events]: \u001b[0m eta: 0:02:19  iter: 2639  total_loss: 0.06621  loss_cls: 0.01447  loss_box_reg: 0.04033  loss_rpn_cls: 0.0006714  loss_rpn_loc: 0.007111  time: 0.3864  data_time: 0.0087  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:59:18 d2.utils.events]: \u001b[0m eta: 0:02:12  iter: 2659  total_loss: 0.07355  loss_cls: 0.02002  loss_box_reg: 0.04238  loss_rpn_cls: 0.003029  loss_rpn_loc: 0.006045  time: 0.3864  data_time: 0.0082  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:59:26 d2.utils.events]: \u001b[0m eta: 0:02:04  iter: 2679  total_loss: 0.07545  loss_cls: 0.01584  loss_box_reg: 0.04075  loss_rpn_cls: 0.001946  loss_rpn_loc: 0.009807  time: 0.3865  data_time: 0.0079  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:59:33 d2.utils.events]: \u001b[0m eta: 0:01:56  iter: 2699  total_loss: 0.07664  loss_cls: 0.02326  loss_box_reg: 0.04347  loss_rpn_cls: 0.001757  loss_rpn_loc: 0.00602  time: 0.3864  data_time: 0.0083  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:59:41 d2.utils.events]: \u001b[0m eta: 0:01:49  iter: 2719  total_loss: 0.05581  loss_cls: 0.01177  loss_box_reg: 0.03516  loss_rpn_cls: 0.000811  loss_rpn_loc: 0.004557  time: 0.3865  data_time: 0.0082  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:59:49 d2.utils.events]: \u001b[0m eta: 0:01:41  iter: 2739  total_loss: 0.05847  loss_cls: 0.01431  loss_box_reg: 0.04115  loss_rpn_cls: 0.001249  loss_rpn_loc: 0.00523  time: 0.3866  data_time: 0.0085  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 19:59:57 d2.utils.events]: \u001b[0m eta: 0:01:33  iter: 2759  total_loss: 0.06287  loss_cls: 0.01459  loss_box_reg: 0.03883  loss_rpn_cls: 0.0008418  loss_rpn_loc: 0.004803  time: 0.3866  data_time: 0.0089  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 20:00:05 d2.utils.events]: \u001b[0m eta: 0:01:25  iter: 2779  total_loss: 0.07214  loss_cls: 0.01764  loss_box_reg: 0.0389  loss_rpn_cls: 0.0008463  loss_rpn_loc: 0.009656  time: 0.3867  data_time: 0.0078  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 20:00:13 d2.utils.events]: \u001b[0m eta: 0:01:18  iter: 2799  total_loss: 0.06728  loss_cls: 0.01592  loss_box_reg: 0.04221  loss_rpn_cls: 0.001096  loss_rpn_loc: 0.006044  time: 0.3867  data_time: 0.0081  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 20:00:21 d2.utils.events]: \u001b[0m eta: 0:01:10  iter: 2819  total_loss: 0.0779  loss_cls: 0.02168  loss_box_reg: 0.04481  loss_rpn_cls: 0.001543  loss_rpn_loc: 0.006788  time: 0.3868  data_time: 0.0085  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 20:00:29 d2.utils.events]: \u001b[0m eta: 0:01:02  iter: 2839  total_loss: 0.09906  loss_cls: 0.03164  loss_box_reg: 0.04958  loss_rpn_cls: 0.002006  loss_rpn_loc: 0.008369  time: 0.3868  data_time: 0.0083  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 20:00:37 d2.utils.events]: \u001b[0m eta: 0:00:54  iter: 2859  total_loss: 0.08296  loss_cls: 0.0243  loss_box_reg: 0.05028  loss_rpn_cls: 0.002444  loss_rpn_loc: 0.006901  time: 0.3868  data_time: 0.0086  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 20:00:45 d2.utils.events]: \u001b[0m eta: 0:00:47  iter: 2879  total_loss: 0.06951  loss_cls: 0.01959  loss_box_reg: 0.04293  loss_rpn_cls: 0.001764  loss_rpn_loc: 0.004106  time: 0.3869  data_time: 0.0077  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 20:00:53 d2.utils.events]: \u001b[0m eta: 0:00:39  iter: 2899  total_loss: 0.07666  loss_cls: 0.01762  loss_box_reg: 0.04762  loss_rpn_cls: 0.002284  loss_rpn_loc: 0.007377  time: 0.3870  data_time: 0.0083  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 20:01:00 d2.utils.events]: \u001b[0m eta: 0:00:31  iter: 2919  total_loss: 0.06498  loss_cls: 0.01717  loss_box_reg: 0.03816  loss_rpn_cls: 0.0008076  loss_rpn_loc: 0.005558  time: 0.3870  data_time: 0.0078  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 20:01:08 d2.utils.events]: \u001b[0m eta: 0:00:23  iter: 2939  total_loss: 0.05939  loss_cls: 0.01475  loss_box_reg: 0.03972  loss_rpn_cls: 0.0007593  loss_rpn_loc: 0.004694  time: 0.3870  data_time: 0.0075  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 20:01:16 d2.utils.events]: \u001b[0m eta: 0:00:15  iter: 2959  total_loss: 0.09357  loss_cls: 0.02785  loss_box_reg: 0.05391  loss_rpn_cls: 0.0009049  loss_rpn_loc: 0.005102  time: 0.3869  data_time: 0.0062  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 20:01:23 d2.utils.events]: \u001b[0m eta: 0:00:07  iter: 2979  total_loss: 0.07296  loss_cls: 0.02449  loss_box_reg: 0.03944  loss_rpn_cls: 0.001347  loss_rpn_loc: 0.006288  time: 0.3869  data_time: 0.0073  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 20:01:33 d2.utils.events]: \u001b[0m eta: 0:00:00  iter: 2999  total_loss: 0.06696  loss_cls: 0.01862  loss_box_reg: 0.04124  loss_rpn_cls: 0.001429  loss_rpn_loc: 0.004177  time: 0.3869  data_time: 0.0086  lr: 0.0125  max_mem: 4691M\n",
      "\u001b[32m[02/01 20:01:33 d2.engine.hooks]: \u001b[0mOverall training speed: 2998 iterations in 0:19:20 (0.3869 s / it)\n",
      "\u001b[32m[02/01 20:01:33 d2.engine.hooks]: \u001b[0mTotal training time: 0:19:25 (0:00:05 on hooks)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./output/latex-smudge/model_final.pth'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type_ = \"/latex-smudge/\"\n",
    "\n",
    "train_set_name = \"my_dataset_train_\" + type_ + \"1\"\n",
    "\n",
    "# Train set\n",
    "DatasetCatalog.register(train_set_name, get_train_latex_with_smudge)\n",
    "MetadataCatalog.get(train_set_name).set(thing_classes=[\"table\"])\n",
    "text_metadata_train = MetadataCatalog.get(train_set_name)\n",
    "\n",
    "cfg = get_cfg()\n",
    "# \"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"\n",
    "# faster_rcnn_X_101_32x8d_FPN_3x.yaml\n",
    "#cfg.merge_from_file(\"/data/rali5/Tmp/yockelle/TableBank/TableBank/output/word/X152/config_word_X152.yaml\")\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.DATASETS.TRAIN = (train_set_name,)\n",
    "cfg.DATASETS.TEST = ()\n",
    "cfg.DATALOADER.NUM_WORKERS = 4\n",
    "cfg.DATALOADER.FILTER_EMPTY_ANNOTATIONS = False\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")  # Let training initialize from model zoo\n",
    "cfg.SOLVER.IMS_PER_BATCH = 4\n",
    "cfg.SOLVER.BASE_LR = 0.0125  # pick a good LR\n",
    "cfg.SOLVER.MAX_ITER = 3000    # 300 iterations seems good enough for this toy dataset; you will need to train longer for a practical dataset\n",
    "cfg.SOLVER.STEPS = []        # do not decay learning rate\n",
    "#cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, and good enough for this toy dataset (default: 512)\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 256\n",
    "cfg.MODEL.PANOPTIC_FPN.COMBINE.INSTANCES_CONFIDENCE_THRESH = 0.5\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n",
    "cfg.TEST.EVAL_PERIOD = 1000\n",
    "CUDA_LAUNCH_BLOCKING = 1\n",
    "\n",
    "os.makedirs(cfg.OUTPUT_DIR + type_, exist_ok=True)\n",
    "#trainer = DefaultTrainer(cfg) \n",
    "trainer = CocoTrainer(cfg) \n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()\n",
    "\n",
    "shutil.move(cfg.OUTPUT_DIR + \"/\" + \"model_final.pth\", cfg.OUTPUT_DIR + type_ + \"model_final.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[01/30 21:14:34 d2.engine.defaults]: \u001b[0mModel:\n",
      "GeneralizedRCNN(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelMaxPool()\n",
      "    (bottom_up): ResNet(\n",
      "      (stem): BasicStem(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (res2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (4): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (5): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res5): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): StandardROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc_relu1): ReLU()\n",
      "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (fc_relu2): ReLU()\n",
      "    )\n",
      "    (box_predictor): FastRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m[01/30 21:16:01 d2.data.build]: \u001b[0mDistribution of instances among all 1 categories:\n",
      "\u001b[36m|  category  | #instances   |\n",
      "|:----------:|:-------------|\n",
      "|   table    | 474862       |\n",
      "|            |              |\u001b[0m\n",
      "\u001b[32m[01/30 21:16:01 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
      "\u001b[32m[01/30 21:16:01 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
      "\u001b[32m[01/30 21:16:01 d2.data.common]: \u001b[0mSerializing 374398 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[01/30 21:16:03 d2.data.common]: \u001b[0mSerialized dataset takes 166.00 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (2, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (2,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (4, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (4,) in the model! You might want to double check if this is expected.\n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_predictor.cls_score.{bias, weight}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[01/30 21:16:04 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/yockelle/anaconda3/lib/python3.8/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[01/30 21:16:12 d2.utils.events]: \u001b[0m eta: 0:17:08  iter: 19  total_loss: 0.6107  loss_cls: 0.3417  loss_box_reg: 0.07973  loss_rpn_cls: 0.06092  loss_rpn_loc: 0.01227  time: 0.3501  data_time: 0.0221  lr: 0.00024976  max_mem: 4333M\n",
      "\u001b[32m[01/30 21:16:19 d2.utils.events]: \u001b[0m eta: 0:17:07  iter: 39  total_loss: 0.4511  loss_cls: 0.1681  loss_box_reg: 0.2424  loss_rpn_cls: 0.02326  loss_rpn_loc: 0.01034  time: 0.3524  data_time: 0.0063  lr: 0.00049951  max_mem: 4333M\n",
      "\u001b[32m[01/30 21:16:26 d2.utils.events]: \u001b[0m eta: 0:17:31  iter: 59  total_loss: 0.3755  loss_cls: 0.1144  loss_box_reg: 0.2437  loss_rpn_cls: 0.01154  loss_rpn_loc: 0.007897  time: 0.3572  data_time: 0.0066  lr: 0.00074926  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:16:33 d2.utils.events]: \u001b[0m eta: 0:17:14  iter: 79  total_loss: 0.3014  loss_cls: 0.0775  loss_box_reg: 0.2157  loss_rpn_cls: 0.005149  loss_rpn_loc: 0.007935  time: 0.3544  data_time: 0.0066  lr: 0.00099901  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:16:41 d2.utils.events]: \u001b[0m eta: 0:17:10  iter: 99  total_loss: 0.2356  loss_cls: 0.07286  loss_box_reg: 0.1384  loss_rpn_cls: 0.004706  loss_rpn_loc: 0.01474  time: 0.3554  data_time: 0.0067  lr: 0.0012488  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:16:48 d2.utils.events]: \u001b[0m eta: 0:17:09  iter: 119  total_loss: 0.1484  loss_cls: 0.05052  loss_box_reg: 0.0854  loss_rpn_cls: 0.002249  loss_rpn_loc: 0.01239  time: 0.3567  data_time: 0.0063  lr: 0.0014985  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:16:55 d2.utils.events]: \u001b[0m eta: 0:16:54  iter: 139  total_loss: 0.1253  loss_cls: 0.04406  loss_box_reg: 0.06785  loss_rpn_cls: 0.002253  loss_rpn_loc: 0.007622  time: 0.3550  data_time: 0.0065  lr: 0.0017483  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:17:02 d2.utils.events]: \u001b[0m eta: 0:16:52  iter: 159  total_loss: 0.1432  loss_cls: 0.04155  loss_box_reg: 0.09408  loss_rpn_cls: 0.00154  loss_rpn_loc: 0.007032  time: 0.3554  data_time: 0.0063  lr: 0.001998  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:17:09 d2.utils.events]: \u001b[0m eta: 0:16:49  iter: 179  total_loss: 0.1315  loss_cls: 0.04575  loss_box_reg: 0.07104  loss_rpn_cls: 0.003178  loss_rpn_loc: 0.008731  time: 0.3562  data_time: 0.0066  lr: 0.0022478  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:17:16 d2.utils.events]: \u001b[0m eta: 0:16:42  iter: 199  total_loss: 0.1164  loss_cls: 0.04331  loss_box_reg: 0.06342  loss_rpn_cls: 0.001674  loss_rpn_loc: 0.006308  time: 0.3564  data_time: 0.0063  lr: 0.0024975  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:17:24 d2.utils.events]: \u001b[0m eta: 0:16:34  iter: 219  total_loss: 0.1224  loss_cls: 0.03772  loss_box_reg: 0.06918  loss_rpn_cls: 0.001818  loss_rpn_loc: 0.00752  time: 0.3560  data_time: 0.0064  lr: 0.0027473  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:17:31 d2.utils.events]: \u001b[0m eta: 0:16:31  iter: 239  total_loss: 0.1131  loss_cls: 0.03669  loss_box_reg: 0.06395  loss_rpn_cls: 0.001119  loss_rpn_loc: 0.006603  time: 0.3563  data_time: 0.0067  lr: 0.002997  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:17:38 d2.utils.events]: \u001b[0m eta: 0:16:27  iter: 259  total_loss: 0.1513  loss_cls: 0.04081  loss_box_reg: 0.09838  loss_rpn_cls: 0.002329  loss_rpn_loc: 0.01086  time: 0.3562  data_time: 0.0064  lr: 0.0032468  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:17:45 d2.utils.events]: \u001b[0m eta: 0:16:21  iter: 279  total_loss: 0.1242  loss_cls: 0.03105  loss_box_reg: 0.08187  loss_rpn_cls: 0.001434  loss_rpn_loc: 0.008503  time: 0.3564  data_time: 0.0065  lr: 0.0034965  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:17:52 d2.utils.events]: \u001b[0m eta: 0:16:14  iter: 299  total_loss: 0.1073  loss_cls: 0.0351  loss_box_reg: 0.06853  loss_rpn_cls: 0.00076  loss_rpn_loc: 0.005257  time: 0.3565  data_time: 0.0063  lr: 0.0037463  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:18:00 d2.utils.events]: \u001b[0m eta: 0:16:08  iter: 319  total_loss: 0.1277  loss_cls: 0.03733  loss_box_reg: 0.0642  loss_rpn_cls: 0.001384  loss_rpn_loc: 0.006327  time: 0.3568  data_time: 0.0067  lr: 0.003996  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:18:07 d2.utils.events]: \u001b[0m eta: 0:16:00  iter: 339  total_loss: 0.1402  loss_cls: 0.04902  loss_box_reg: 0.06598  loss_rpn_cls: 0.001923  loss_rpn_loc: 0.007667  time: 0.3567  data_time: 0.0068  lr: 0.0042458  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:18:14 d2.utils.events]: \u001b[0m eta: 0:15:46  iter: 359  total_loss: 0.1285  loss_cls: 0.0367  loss_box_reg: 0.07008  loss_rpn_cls: 0.0007956  loss_rpn_loc: 0.009219  time: 0.3565  data_time: 0.0068  lr: 0.0044955  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:18:21 d2.utils.events]: \u001b[0m eta: 0:15:40  iter: 379  total_loss: 0.101  loss_cls: 0.03095  loss_box_reg: 0.06072  loss_rpn_cls: 0.0007407  loss_rpn_loc: 0.005427  time: 0.3564  data_time: 0.0065  lr: 0.0047453  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:18:28 d2.utils.events]: \u001b[0m eta: 0:15:37  iter: 399  total_loss: 0.1197  loss_cls: 0.03812  loss_box_reg: 0.07118  loss_rpn_cls: 0.001678  loss_rpn_loc: 0.01036  time: 0.3567  data_time: 0.0064  lr: 0.004995  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:18:35 d2.utils.events]: \u001b[0m eta: 0:15:29  iter: 419  total_loss: 0.1138  loss_cls: 0.03259  loss_box_reg: 0.06407  loss_rpn_cls: 0.00165  loss_rpn_loc: 0.007328  time: 0.3566  data_time: 0.0065  lr: 0.0052448  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:18:42 d2.utils.events]: \u001b[0m eta: 0:15:24  iter: 439  total_loss: 0.1194  loss_cls: 0.03947  loss_box_reg: 0.05708  loss_rpn_cls: 0.001847  loss_rpn_loc: 0.007162  time: 0.3568  data_time: 0.0066  lr: 0.0054945  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:18:50 d2.utils.events]: \u001b[0m eta: 0:15:17  iter: 459  total_loss: 0.1438  loss_cls: 0.0411  loss_box_reg: 0.07819  loss_rpn_cls: 0.001292  loss_rpn_loc: 0.01136  time: 0.3567  data_time: 0.0064  lr: 0.0057443  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:18:57 d2.utils.events]: \u001b[0m eta: 0:15:09  iter: 479  total_loss: 0.1364  loss_cls: 0.04458  loss_box_reg: 0.06816  loss_rpn_cls: 0.008766  loss_rpn_loc: 0.01111  time: 0.3564  data_time: 0.0066  lr: 0.005994  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:19:04 d2.utils.events]: \u001b[0m eta: 0:15:02  iter: 499  total_loss: 0.1393  loss_cls: 0.04105  loss_box_reg: 0.06799  loss_rpn_cls: 0.003453  loss_rpn_loc: 0.01017  time: 0.3562  data_time: 0.0063  lr: 0.0062438  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:19:11 d2.utils.events]: \u001b[0m eta: 0:14:54  iter: 519  total_loss: 0.09253  loss_cls: 0.02463  loss_box_reg: 0.05472  loss_rpn_cls: 0.001472  loss_rpn_loc: 0.008009  time: 0.3559  data_time: 0.0064  lr: 0.0064935  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:19:18 d2.utils.events]: \u001b[0m eta: 0:14:47  iter: 539  total_loss: 0.0898  loss_cls: 0.02672  loss_box_reg: 0.05195  loss_rpn_cls: 0.0008799  loss_rpn_loc: 0.009693  time: 0.3560  data_time: 0.0066  lr: 0.0067433  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:19:25 d2.utils.events]: \u001b[0m eta: 0:14:40  iter: 559  total_loss: 0.08607  loss_cls: 0.02287  loss_box_reg: 0.04578  loss_rpn_cls: 0.001339  loss_rpn_loc: 0.00513  time: 0.3558  data_time: 0.0065  lr: 0.006993  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:19:32 d2.utils.events]: \u001b[0m eta: 0:14:34  iter: 579  total_loss: 0.1072  loss_cls: 0.03542  loss_box_reg: 0.05343  loss_rpn_cls: 0.002895  loss_rpn_loc: 0.006652  time: 0.3560  data_time: 0.0063  lr: 0.0072428  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:19:39 d2.utils.events]: \u001b[0m eta: 0:14:27  iter: 599  total_loss: 0.1052  loss_cls: 0.03317  loss_box_reg: 0.05506  loss_rpn_cls: 0.003719  loss_rpn_loc: 0.01216  time: 0.3562  data_time: 0.0066  lr: 0.0074925  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:19:46 d2.utils.events]: \u001b[0m eta: 0:14:20  iter: 619  total_loss: 0.1205  loss_cls: 0.03205  loss_box_reg: 0.06236  loss_rpn_cls: 0.002561  loss_rpn_loc: 0.00858  time: 0.3561  data_time: 0.0065  lr: 0.0077423  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:19:54 d2.utils.events]: \u001b[0m eta: 0:14:13  iter: 639  total_loss: 0.1207  loss_cls: 0.03653  loss_box_reg: 0.07077  loss_rpn_cls: 0.001493  loss_rpn_loc: 0.009176  time: 0.3562  data_time: 0.0067  lr: 0.007992  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:20:01 d2.utils.events]: \u001b[0m eta: 0:14:06  iter: 659  total_loss: 0.1159  loss_cls: 0.03568  loss_box_reg: 0.06436  loss_rpn_cls: 0.001133  loss_rpn_loc: 0.00814  time: 0.3563  data_time: 0.0065  lr: 0.0082418  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:20:08 d2.utils.events]: \u001b[0m eta: 0:13:59  iter: 679  total_loss: 0.1031  loss_cls: 0.03699  loss_box_reg: 0.05677  loss_rpn_cls: 0.00143  loss_rpn_loc: 0.008488  time: 0.3564  data_time: 0.0068  lr: 0.0084915  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:20:15 d2.utils.events]: \u001b[0m eta: 0:13:50  iter: 699  total_loss: 0.1092  loss_cls: 0.03442  loss_box_reg: 0.06095  loss_rpn_cls: 0.001676  loss_rpn_loc: 0.01207  time: 0.3564  data_time: 0.0066  lr: 0.0087413  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:20:23 d2.utils.events]: \u001b[0m eta: 0:13:45  iter: 719  total_loss: 0.0949  loss_cls: 0.02766  loss_box_reg: 0.05243  loss_rpn_cls: 0.001922  loss_rpn_loc: 0.007248  time: 0.3566  data_time: 0.0065  lr: 0.008991  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:20:29 d2.utils.events]: \u001b[0m eta: 0:13:37  iter: 739  total_loss: 0.1098  loss_cls: 0.02643  loss_box_reg: 0.06525  loss_rpn_cls: 0.002662  loss_rpn_loc: 0.00679  time: 0.3563  data_time: 0.0062  lr: 0.0092408  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:20:36 d2.utils.events]: \u001b[0m eta: 0:13:29  iter: 759  total_loss: 0.1104  loss_cls: 0.03851  loss_box_reg: 0.06775  loss_rpn_cls: 0.003591  loss_rpn_loc: 0.00843  time: 0.3561  data_time: 0.0066  lr: 0.0094905  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:20:43 d2.utils.events]: \u001b[0m eta: 0:13:21  iter: 779  total_loss: 0.1183  loss_cls: 0.04312  loss_box_reg: 0.06519  loss_rpn_cls: 0.001933  loss_rpn_loc: 0.005966  time: 0.3560  data_time: 0.0065  lr: 0.0097403  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:20:51 d2.utils.events]: \u001b[0m eta: 0:13:14  iter: 799  total_loss: 0.08818  loss_cls: 0.02387  loss_box_reg: 0.05231  loss_rpn_cls: 0.0008129  loss_rpn_loc: 0.008997  time: 0.3559  data_time: 0.0063  lr: 0.00999  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:20:58 d2.utils.events]: \u001b[0m eta: 0:13:03  iter: 819  total_loss: 0.1019  loss_cls: 0.02605  loss_box_reg: 0.0633  loss_rpn_cls: 0.001558  loss_rpn_loc: 0.00668  time: 0.3557  data_time: 0.0066  lr: 0.01024  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:21:05 d2.utils.events]: \u001b[0m eta: 0:12:55  iter: 839  total_loss: 0.1008  loss_cls: 0.03168  loss_box_reg: 0.06154  loss_rpn_cls: 0.001055  loss_rpn_loc: 0.005594  time: 0.3555  data_time: 0.0067  lr: 0.01049  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:21:12 d2.utils.events]: \u001b[0m eta: 0:12:49  iter: 859  total_loss: 0.08523  loss_cls: 0.02146  loss_box_reg: 0.04721  loss_rpn_cls: 0.0008165  loss_rpn_loc: 0.006404  time: 0.3556  data_time: 0.0065  lr: 0.010739  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:21:19 d2.utils.events]: \u001b[0m eta: 0:12:41  iter: 879  total_loss: 0.09943  loss_cls: 0.02742  loss_box_reg: 0.05751  loss_rpn_cls: 0.002625  loss_rpn_loc: 0.008304  time: 0.3556  data_time: 0.0061  lr: 0.010989  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:21:26 d2.utils.events]: \u001b[0m eta: 0:12:34  iter: 899  total_loss: 0.1039  loss_cls: 0.02545  loss_box_reg: 0.05599  loss_rpn_cls: 0.001073  loss_rpn_loc: 0.007139  time: 0.3556  data_time: 0.0067  lr: 0.011239  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:21:33 d2.utils.events]: \u001b[0m eta: 0:12:26  iter: 919  total_loss: 0.08998  loss_cls: 0.02445  loss_box_reg: 0.05377  loss_rpn_cls: 0.00107  loss_rpn_loc: 0.008643  time: 0.3556  data_time: 0.0064  lr: 0.011489  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:21:40 d2.utils.events]: \u001b[0m eta: 0:12:20  iter: 939  total_loss: 0.08997  loss_cls: 0.02396  loss_box_reg: 0.05364  loss_rpn_cls: 0.001668  loss_rpn_loc: 0.009668  time: 0.3558  data_time: 0.0064  lr: 0.011738  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:21:48 d2.utils.events]: \u001b[0m eta: 0:12:13  iter: 959  total_loss: 0.1134  loss_cls: 0.03431  loss_box_reg: 0.06792  loss_rpn_cls: 0.001397  loss_rpn_loc: 0.008244  time: 0.3559  data_time: 0.0069  lr: 0.011988  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:21:55 d2.utils.events]: \u001b[0m eta: 0:12:05  iter: 979  total_loss: 0.08364  loss_cls: 0.02214  loss_box_reg: 0.05284  loss_rpn_cls: 0.001224  loss_rpn_loc: 0.006614  time: 0.3559  data_time: 0.0065  lr: 0.012238  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:22:02 d2.utils.events]: \u001b[0m eta: 0:11:59  iter: 999  total_loss: 0.09674  loss_cls: 0.02463  loss_box_reg: 0.0528  loss_rpn_cls: 0.00111  loss_rpn_loc: 0.008559  time: 0.3561  data_time: 0.0066  lr: 0.012488  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:22:09 d2.utils.events]: \u001b[0m eta: 0:11:52  iter: 1019  total_loss: 0.2022  loss_cls: 0.08502  loss_box_reg: 0.08546  loss_rpn_cls: 0.003074  loss_rpn_loc: 0.01273  time: 0.3561  data_time: 0.0063  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:22:17 d2.utils.events]: \u001b[0m eta: 0:11:47  iter: 1039  total_loss: 0.1502  loss_cls: 0.04819  loss_box_reg: 0.08282  loss_rpn_cls: 0.007003  loss_rpn_loc: 0.008675  time: 0.3564  data_time: 0.0066  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:22:24 d2.utils.events]: \u001b[0m eta: 0:11:40  iter: 1059  total_loss: 0.1208  loss_cls: 0.0393  loss_box_reg: 0.06835  loss_rpn_cls: 0.00223  loss_rpn_loc: 0.008951  time: 0.3567  data_time: 0.0066  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:22:31 d2.utils.events]: \u001b[0m eta: 0:11:33  iter: 1079  total_loss: 0.1248  loss_cls: 0.03352  loss_box_reg: 0.06389  loss_rpn_cls: 0.00246  loss_rpn_loc: 0.009552  time: 0.3568  data_time: 0.0066  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:22:39 d2.utils.events]: \u001b[0m eta: 0:11:26  iter: 1099  total_loss: 0.1224  loss_cls: 0.03647  loss_box_reg: 0.07096  loss_rpn_cls: 0.001517  loss_rpn_loc: 0.006545  time: 0.3568  data_time: 0.0064  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:22:46 d2.utils.events]: \u001b[0m eta: 0:11:18  iter: 1119  total_loss: 0.09688  loss_cls: 0.03016  loss_box_reg: 0.05075  loss_rpn_cls: 0.00914  loss_rpn_loc: 0.00745  time: 0.3568  data_time: 0.0065  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:22:53 d2.utils.events]: \u001b[0m eta: 0:11:12  iter: 1139  total_loss: 0.15  loss_cls: 0.0519  loss_box_reg: 0.07001  loss_rpn_cls: 0.005189  loss_rpn_loc: 0.01065  time: 0.3570  data_time: 0.0066  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:23:00 d2.utils.events]: \u001b[0m eta: 0:11:04  iter: 1159  total_loss: 0.1313  loss_cls: 0.03742  loss_box_reg: 0.0657  loss_rpn_cls: 0.002623  loss_rpn_loc: 0.008449  time: 0.3570  data_time: 0.0064  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:23:08 d2.utils.events]: \u001b[0m eta: 0:10:56  iter: 1179  total_loss: 0.09192  loss_cls: 0.03017  loss_box_reg: 0.05219  loss_rpn_cls: 0.001536  loss_rpn_loc: 0.005998  time: 0.3570  data_time: 0.0063  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:23:15 d2.utils.events]: \u001b[0m eta: 0:10:49  iter: 1199  total_loss: 0.1187  loss_cls: 0.03261  loss_box_reg: 0.06223  loss_rpn_cls: 0.009153  loss_rpn_loc: 0.008769  time: 0.3572  data_time: 0.0067  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:23:22 d2.utils.events]: \u001b[0m eta: 0:10:43  iter: 1219  total_loss: 0.1254  loss_cls: 0.04101  loss_box_reg: 0.06806  loss_rpn_cls: 0.004614  loss_rpn_loc: 0.009569  time: 0.3573  data_time: 0.0068  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:23:29 d2.utils.events]: \u001b[0m eta: 0:10:35  iter: 1239  total_loss: 0.1276  loss_cls: 0.03763  loss_box_reg: 0.06177  loss_rpn_cls: 0.006329  loss_rpn_loc: 0.008882  time: 0.3573  data_time: 0.0066  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:23:37 d2.utils.events]: \u001b[0m eta: 0:10:28  iter: 1259  total_loss: 0.1108  loss_cls: 0.02891  loss_box_reg: 0.06781  loss_rpn_cls: 0.003071  loss_rpn_loc: 0.007623  time: 0.3574  data_time: 0.0063  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:23:44 d2.utils.events]: \u001b[0m eta: 0:10:22  iter: 1279  total_loss: 0.1061  loss_cls: 0.02669  loss_box_reg: 0.06629  loss_rpn_cls: 0.002012  loss_rpn_loc: 0.007307  time: 0.3577  data_time: 0.0065  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:23:51 d2.utils.events]: \u001b[0m eta: 0:10:15  iter: 1299  total_loss: 0.1012  loss_cls: 0.02848  loss_box_reg: 0.06001  loss_rpn_cls: 0.001899  loss_rpn_loc: 0.00731  time: 0.3578  data_time: 0.0067  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:23:59 d2.utils.events]: \u001b[0m eta: 0:10:08  iter: 1319  total_loss: 0.1075  loss_cls: 0.03445  loss_box_reg: 0.0608  loss_rpn_cls: 0.001754  loss_rpn_loc: 0.005699  time: 0.3579  data_time: 0.0066  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:24:06 d2.utils.events]: \u001b[0m eta: 0:10:02  iter: 1339  total_loss: 0.0847  loss_cls: 0.02077  loss_box_reg: 0.04985  loss_rpn_cls: 0.002233  loss_rpn_loc: 0.009404  time: 0.3579  data_time: 0.0067  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:24:13 d2.utils.events]: \u001b[0m eta: 0:09:55  iter: 1359  total_loss: 0.09349  loss_cls: 0.02671  loss_box_reg: 0.05574  loss_rpn_cls: 0.00173  loss_rpn_loc: 0.007772  time: 0.3581  data_time: 0.0065  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:24:21 d2.utils.events]: \u001b[0m eta: 0:09:48  iter: 1379  total_loss: 0.08144  loss_cls: 0.02297  loss_box_reg: 0.04867  loss_rpn_cls: 0.001568  loss_rpn_loc: 0.006898  time: 0.3582  data_time: 0.0066  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:24:28 d2.utils.events]: \u001b[0m eta: 0:09:41  iter: 1399  total_loss: 0.09727  loss_cls: 0.02573  loss_box_reg: 0.0536  loss_rpn_cls: 0.001176  loss_rpn_loc: 0.007411  time: 0.3583  data_time: 0.0064  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:24:36 d2.utils.events]: \u001b[0m eta: 0:09:34  iter: 1419  total_loss: 0.1068  loss_cls: 0.02252  loss_box_reg: 0.06581  loss_rpn_cls: 0.00207  loss_rpn_loc: 0.01122  time: 0.3584  data_time: 0.0068  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:24:43 d2.utils.events]: \u001b[0m eta: 0:09:27  iter: 1439  total_loss: 0.1141  loss_cls: 0.04036  loss_box_reg: 0.05534  loss_rpn_cls: 0.004972  loss_rpn_loc: 0.01351  time: 0.3586  data_time: 0.0068  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:24:50 d2.utils.events]: \u001b[0m eta: 0:09:19  iter: 1459  total_loss: 0.114  loss_cls: 0.03041  loss_box_reg: 0.06318  loss_rpn_cls: 0.003149  loss_rpn_loc: 0.01007  time: 0.3586  data_time: 0.0062  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:24:58 d2.utils.events]: \u001b[0m eta: 0:09:13  iter: 1479  total_loss: 0.07882  loss_cls: 0.01929  loss_box_reg: 0.04578  loss_rpn_cls: 0.001603  loss_rpn_loc: 0.00805  time: 0.3588  data_time: 0.0067  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:25:05 d2.utils.events]: \u001b[0m eta: 0:09:06  iter: 1499  total_loss: 0.08524  loss_cls: 0.0239  loss_box_reg: 0.04672  loss_rpn_cls: 0.001875  loss_rpn_loc: 0.004875  time: 0.3590  data_time: 0.0067  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:25:13 d2.utils.events]: \u001b[0m eta: 0:08:59  iter: 1519  total_loss: 0.09305  loss_cls: 0.02347  loss_box_reg: 0.05122  loss_rpn_cls: 0.001241  loss_rpn_loc: 0.007948  time: 0.3591  data_time: 0.0067  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:25:20 d2.utils.events]: \u001b[0m eta: 0:08:52  iter: 1539  total_loss: 0.08339  loss_cls: 0.02096  loss_box_reg: 0.0475  loss_rpn_cls: 0.001346  loss_rpn_loc: 0.007135  time: 0.3591  data_time: 0.0065  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:25:27 d2.utils.events]: \u001b[0m eta: 0:08:45  iter: 1559  total_loss: 0.1013  loss_cls: 0.03205  loss_box_reg: 0.05638  loss_rpn_cls: 0.001671  loss_rpn_loc: 0.007159  time: 0.3591  data_time: 0.0066  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:25:34 d2.utils.events]: \u001b[0m eta: 0:08:37  iter: 1579  total_loss: 0.08268  loss_cls: 0.01975  loss_box_reg: 0.04907  loss_rpn_cls: 0.001105  loss_rpn_loc: 0.006961  time: 0.3592  data_time: 0.0065  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:25:42 d2.utils.events]: \u001b[0m eta: 0:08:30  iter: 1599  total_loss: 0.09472  loss_cls: 0.02242  loss_box_reg: 0.04576  loss_rpn_cls: 0.002829  loss_rpn_loc: 0.006163  time: 0.3591  data_time: 0.0062  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:25:49 d2.utils.events]: \u001b[0m eta: 0:08:22  iter: 1619  total_loss: 0.09926  loss_cls: 0.02642  loss_box_reg: 0.06416  loss_rpn_cls: 0.001743  loss_rpn_loc: 0.007394  time: 0.3591  data_time: 0.0068  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:25:56 d2.utils.events]: \u001b[0m eta: 0:08:15  iter: 1639  total_loss: 0.07247  loss_cls: 0.02377  loss_box_reg: 0.04564  loss_rpn_cls: 0.001222  loss_rpn_loc: 0.006859  time: 0.3592  data_time: 0.0065  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:26:03 d2.utils.events]: \u001b[0m eta: 0:08:08  iter: 1659  total_loss: 0.07669  loss_cls: 0.02146  loss_box_reg: 0.04482  loss_rpn_cls: 0.001191  loss_rpn_loc: 0.007487  time: 0.3592  data_time: 0.0064  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:26:11 d2.utils.events]: \u001b[0m eta: 0:08:01  iter: 1679  total_loss: 0.07924  loss_cls: 0.02129  loss_box_reg: 0.04663  loss_rpn_cls: 0.0009712  loss_rpn_loc: 0.005031  time: 0.3593  data_time: 0.0063  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:26:18 d2.utils.events]: \u001b[0m eta: 0:07:54  iter: 1699  total_loss: 0.09244  loss_cls: 0.02311  loss_box_reg: 0.05956  loss_rpn_cls: 0.000545  loss_rpn_loc: 0.005462  time: 0.3594  data_time: 0.0065  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:26:25 d2.utils.events]: \u001b[0m eta: 0:07:46  iter: 1719  total_loss: 0.1007  loss_cls: 0.02554  loss_box_reg: 0.06002  loss_rpn_cls: 0.001537  loss_rpn_loc: 0.00654  time: 0.3595  data_time: 0.0067  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:26:33 d2.utils.events]: \u001b[0m eta: 0:07:39  iter: 1739  total_loss: 0.07207  loss_cls: 0.01604  loss_box_reg: 0.03991  loss_rpn_cls: 0.001654  loss_rpn_loc: 0.01228  time: 0.3595  data_time: 0.0065  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:26:40 d2.utils.events]: \u001b[0m eta: 0:07:32  iter: 1759  total_loss: 0.07276  loss_cls: 0.01963  loss_box_reg: 0.04332  loss_rpn_cls: 0.001321  loss_rpn_loc: 0.007048  time: 0.3596  data_time: 0.0067  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:26:47 d2.utils.events]: \u001b[0m eta: 0:07:26  iter: 1779  total_loss: 0.06864  loss_cls: 0.01502  loss_box_reg: 0.04333  loss_rpn_cls: 0.0004033  loss_rpn_loc: 0.005037  time: 0.3596  data_time: 0.0065  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:26:54 d2.utils.events]: \u001b[0m eta: 0:07:19  iter: 1799  total_loss: 0.07716  loss_cls: 0.02092  loss_box_reg: 0.04906  loss_rpn_cls: 0.001307  loss_rpn_loc: 0.006799  time: 0.3596  data_time: 0.0066  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:27:02 d2.utils.events]: \u001b[0m eta: 0:07:12  iter: 1819  total_loss: 0.08562  loss_cls: 0.01796  loss_box_reg: 0.04591  loss_rpn_cls: 0.001328  loss_rpn_loc: 0.006208  time: 0.3597  data_time: 0.0064  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:27:09 d2.utils.events]: \u001b[0m eta: 0:07:06  iter: 1839  total_loss: 0.08978  loss_cls: 0.02019  loss_box_reg: 0.05714  loss_rpn_cls: 0.000705  loss_rpn_loc: 0.006443  time: 0.3597  data_time: 0.0064  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:27:16 d2.utils.events]: \u001b[0m eta: 0:06:58  iter: 1859  total_loss: 0.07471  loss_cls: 0.01963  loss_box_reg: 0.04923  loss_rpn_cls: 0.0009576  loss_rpn_loc: 0.006407  time: 0.3598  data_time: 0.0065  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:27:24 d2.utils.events]: \u001b[0m eta: 0:06:51  iter: 1879  total_loss: 0.08704  loss_cls: 0.0258  loss_box_reg: 0.0485  loss_rpn_cls: 0.001753  loss_rpn_loc: 0.006846  time: 0.3598  data_time: 0.0066  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:27:31 d2.utils.events]: \u001b[0m eta: 0:06:45  iter: 1899  total_loss: 0.08918  loss_cls: 0.02494  loss_box_reg: 0.05512  loss_rpn_cls: 0.0006257  loss_rpn_loc: 0.006522  time: 0.3600  data_time: 0.0065  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:27:38 d2.utils.events]: \u001b[0m eta: 0:06:37  iter: 1919  total_loss: 0.07249  loss_cls: 0.01944  loss_box_reg: 0.04493  loss_rpn_cls: 0.0008475  loss_rpn_loc: 0.006777  time: 0.3600  data_time: 0.0065  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:27:46 d2.utils.events]: \u001b[0m eta: 0:06:30  iter: 1939  total_loss: 0.1006  loss_cls: 0.02368  loss_box_reg: 0.05926  loss_rpn_cls: 0.001327  loss_rpn_loc: 0.008485  time: 0.3600  data_time: 0.0064  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:27:53 d2.utils.events]: \u001b[0m eta: 0:06:23  iter: 1959  total_loss: 0.1212  loss_cls: 0.03689  loss_box_reg: 0.0701  loss_rpn_cls: 0.001276  loss_rpn_loc: 0.008839  time: 0.3600  data_time: 0.0063  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:28:00 d2.utils.events]: \u001b[0m eta: 0:06:15  iter: 1979  total_loss: 0.08679  loss_cls: 0.02628  loss_box_reg: 0.05002  loss_rpn_cls: 0.001041  loss_rpn_loc: 0.005969  time: 0.3601  data_time: 0.0065  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:28:08 d2.utils.events]: \u001b[0m eta: 0:06:08  iter: 1999  total_loss: 0.09742  loss_cls: 0.02347  loss_box_reg: 0.0543  loss_rpn_cls: 0.0008267  loss_rpn_loc: 0.008428  time: 0.3602  data_time: 0.0065  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:28:15 d2.utils.events]: \u001b[0m eta: 0:06:01  iter: 2019  total_loss: 0.08603  loss_cls: 0.02087  loss_box_reg: 0.04969  loss_rpn_cls: 0.001099  loss_rpn_loc: 0.005691  time: 0.3602  data_time: 0.0063  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:28:22 d2.utils.events]: \u001b[0m eta: 0:05:54  iter: 2039  total_loss: 0.09669  loss_cls: 0.0257  loss_box_reg: 0.0563  loss_rpn_cls: 0.003577  loss_rpn_loc: 0.006655  time: 0.3602  data_time: 0.0064  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:28:30 d2.utils.events]: \u001b[0m eta: 0:05:46  iter: 2059  total_loss: 0.07772  loss_cls: 0.01948  loss_box_reg: 0.04839  loss_rpn_cls: 0.002053  loss_rpn_loc: 0.005755  time: 0.3603  data_time: 0.0064  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:28:37 d2.utils.events]: \u001b[0m eta: 0:05:39  iter: 2079  total_loss: 0.08859  loss_cls: 0.02233  loss_box_reg: 0.05315  loss_rpn_cls: 0.0021  loss_rpn_loc: 0.008959  time: 0.3602  data_time: 0.0062  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:28:44 d2.utils.events]: \u001b[0m eta: 0:05:31  iter: 2099  total_loss: 0.08955  loss_cls: 0.02338  loss_box_reg: 0.05442  loss_rpn_cls: 0.001244  loss_rpn_loc: 0.007466  time: 0.3603  data_time: 0.0067  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:28:51 d2.utils.events]: \u001b[0m eta: 0:05:24  iter: 2119  total_loss: 0.08806  loss_cls: 0.02888  loss_box_reg: 0.05044  loss_rpn_cls: 0.001044  loss_rpn_loc: 0.008318  time: 0.3604  data_time: 0.0066  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:28:59 d2.utils.events]: \u001b[0m eta: 0:05:17  iter: 2139  total_loss: 0.08009  loss_cls: 0.02677  loss_box_reg: 0.04829  loss_rpn_cls: 0.001531  loss_rpn_loc: 0.006962  time: 0.3603  data_time: 0.0064  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:29:06 d2.utils.events]: \u001b[0m eta: 0:05:10  iter: 2159  total_loss: 0.09845  loss_cls: 0.03473  loss_box_reg: 0.04998  loss_rpn_cls: 0.003579  loss_rpn_loc: 0.01332  time: 0.3604  data_time: 0.0064  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:29:13 d2.utils.events]: \u001b[0m eta: 0:05:02  iter: 2179  total_loss: 0.07176  loss_cls: 0.01866  loss_box_reg: 0.03887  loss_rpn_cls: 0.00153  loss_rpn_loc: 0.00711  time: 0.3605  data_time: 0.0065  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:29:20 d2.utils.events]: \u001b[0m eta: 0:04:55  iter: 2199  total_loss: 0.07587  loss_cls: 0.01786  loss_box_reg: 0.04897  loss_rpn_cls: 0.001165  loss_rpn_loc: 0.006095  time: 0.3604  data_time: 0.0070  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:29:28 d2.utils.events]: \u001b[0m eta: 0:04:47  iter: 2219  total_loss: 0.07011  loss_cls: 0.01738  loss_box_reg: 0.04546  loss_rpn_cls: 0.001228  loss_rpn_loc: 0.004677  time: 0.3605  data_time: 0.0067  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:29:35 d2.utils.events]: \u001b[0m eta: 0:04:40  iter: 2239  total_loss: 0.07984  loss_cls: 0.0217  loss_box_reg: 0.04862  loss_rpn_cls: 0.001154  loss_rpn_loc: 0.007228  time: 0.3605  data_time: 0.0066  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:29:42 d2.utils.events]: \u001b[0m eta: 0:04:33  iter: 2259  total_loss: 0.09777  loss_cls: 0.03076  loss_box_reg: 0.05551  loss_rpn_cls: 0.001492  loss_rpn_loc: 0.005529  time: 0.3606  data_time: 0.0067  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:29:50 d2.utils.events]: \u001b[0m eta: 0:04:25  iter: 2279  total_loss: 0.1059  loss_cls: 0.02987  loss_box_reg: 0.05943  loss_rpn_cls: 0.002151  loss_rpn_loc: 0.006256  time: 0.3606  data_time: 0.0066  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:29:57 d2.utils.events]: \u001b[0m eta: 0:04:18  iter: 2299  total_loss: 0.08257  loss_cls: 0.02271  loss_box_reg: 0.04675  loss_rpn_cls: 0.001064  loss_rpn_loc: 0.005852  time: 0.3607  data_time: 0.0065  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:30:05 d2.utils.events]: \u001b[0m eta: 0:04:11  iter: 2319  total_loss: 0.06958  loss_cls: 0.01609  loss_box_reg: 0.04211  loss_rpn_cls: 0.0008552  loss_rpn_loc: 0.006916  time: 0.3607  data_time: 0.0066  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:30:12 d2.utils.events]: \u001b[0m eta: 0:04:03  iter: 2339  total_loss: 0.08861  loss_cls: 0.02429  loss_box_reg: 0.04909  loss_rpn_cls: 0.001101  loss_rpn_loc: 0.007054  time: 0.3607  data_time: 0.0065  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:30:19 d2.utils.events]: \u001b[0m eta: 0:03:55  iter: 2359  total_loss: 0.07589  loss_cls: 0.01881  loss_box_reg: 0.04773  loss_rpn_cls: 0.001485  loss_rpn_loc: 0.007329  time: 0.3607  data_time: 0.0066  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:30:26 d2.utils.events]: \u001b[0m eta: 0:03:48  iter: 2379  total_loss: 0.1068  loss_cls: 0.03607  loss_box_reg: 0.05453  loss_rpn_cls: 0.00223  loss_rpn_loc: 0.008736  time: 0.3607  data_time: 0.0065  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:30:34 d2.utils.events]: \u001b[0m eta: 0:03:41  iter: 2399  total_loss: 0.07018  loss_cls: 0.01918  loss_box_reg: 0.03991  loss_rpn_cls: 0.005214  loss_rpn_loc: 0.005764  time: 0.3608  data_time: 0.0065  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:30:41 d2.utils.events]: \u001b[0m eta: 0:03:33  iter: 2419  total_loss: 0.0655  loss_cls: 0.0174  loss_box_reg: 0.03487  loss_rpn_cls: 0.004552  loss_rpn_loc: 0.008119  time: 0.3609  data_time: 0.0065  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:30:48 d2.utils.events]: \u001b[0m eta: 0:03:26  iter: 2439  total_loss: 0.08013  loss_cls: 0.01861  loss_box_reg: 0.04249  loss_rpn_cls: 0.002372  loss_rpn_loc: 0.007493  time: 0.3610  data_time: 0.0064  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:30:56 d2.utils.events]: \u001b[0m eta: 0:03:19  iter: 2459  total_loss: 0.07169  loss_cls: 0.01433  loss_box_reg: 0.04148  loss_rpn_cls: 0.001315  loss_rpn_loc: 0.007414  time: 0.3610  data_time: 0.0067  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:31:03 d2.utils.events]: \u001b[0m eta: 0:03:11  iter: 2479  total_loss: 0.07842  loss_cls: 0.01802  loss_box_reg: 0.04059  loss_rpn_cls: 0.001549  loss_rpn_loc: 0.006664  time: 0.3612  data_time: 0.0068  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:31:11 d2.utils.events]: \u001b[0m eta: 0:03:04  iter: 2499  total_loss: 0.06319  loss_cls: 0.01461  loss_box_reg: 0.03898  loss_rpn_cls: 0.00109  loss_rpn_loc: 0.005984  time: 0.3613  data_time: 0.0065  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:31:18 d2.utils.events]: \u001b[0m eta: 0:02:56  iter: 2519  total_loss: 0.06627  loss_cls: 0.01606  loss_box_reg: 0.04069  loss_rpn_cls: 0.001421  loss_rpn_loc: 0.005634  time: 0.3614  data_time: 0.0065  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:31:26 d2.utils.events]: \u001b[0m eta: 0:02:49  iter: 2539  total_loss: 0.08309  loss_cls: 0.0306  loss_box_reg: 0.03418  loss_rpn_cls: 0.002877  loss_rpn_loc: 0.007119  time: 0.3614  data_time: 0.0063  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:31:33 d2.utils.events]: \u001b[0m eta: 0:02:42  iter: 2559  total_loss: 0.06826  loss_cls: 0.01661  loss_box_reg: 0.03344  loss_rpn_cls: 0.005859  loss_rpn_loc: 0.006517  time: 0.3614  data_time: 0.0066  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:31:40 d2.utils.events]: \u001b[0m eta: 0:02:34  iter: 2579  total_loss: 0.07074  loss_cls: 0.02121  loss_box_reg: 0.0364  loss_rpn_cls: 0.004105  loss_rpn_loc: 0.007646  time: 0.3614  data_time: 0.0061  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:31:47 d2.utils.events]: \u001b[0m eta: 0:02:27  iter: 2599  total_loss: 0.08142  loss_cls: 0.02347  loss_box_reg: 0.05354  loss_rpn_cls: 0.002879  loss_rpn_loc: 0.004795  time: 0.3614  data_time: 0.0065  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:31:55 d2.utils.events]: \u001b[0m eta: 0:02:20  iter: 2619  total_loss: 0.08124  loss_cls: 0.02123  loss_box_reg: 0.05248  loss_rpn_cls: 0.002786  loss_rpn_loc: 0.006274  time: 0.3614  data_time: 0.0066  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:32:02 d2.utils.events]: \u001b[0m eta: 0:02:13  iter: 2639  total_loss: 0.07004  loss_cls: 0.02246  loss_box_reg: 0.03769  loss_rpn_cls: 0.003472  loss_rpn_loc: 0.007738  time: 0.3614  data_time: 0.0064  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:32:09 d2.utils.events]: \u001b[0m eta: 0:02:05  iter: 2659  total_loss: 0.08049  loss_cls: 0.01929  loss_box_reg: 0.0496  loss_rpn_cls: 0.00208  loss_rpn_loc: 0.005534  time: 0.3615  data_time: 0.0065  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:32:17 d2.utils.events]: \u001b[0m eta: 0:01:58  iter: 2679  total_loss: 0.09323  loss_cls: 0.02159  loss_box_reg: 0.04699  loss_rpn_cls: 0.002049  loss_rpn_loc: 0.006517  time: 0.3615  data_time: 0.0065  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:32:24 d2.utils.events]: \u001b[0m eta: 0:01:50  iter: 2699  total_loss: 0.07411  loss_cls: 0.02105  loss_box_reg: 0.03616  loss_rpn_cls: 0.00152  loss_rpn_loc: 0.005422  time: 0.3616  data_time: 0.0064  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:32:31 d2.utils.events]: \u001b[0m eta: 0:01:43  iter: 2719  total_loss: 0.06366  loss_cls: 0.01914  loss_box_reg: 0.04025  loss_rpn_cls: 0.001416  loss_rpn_loc: 0.00559  time: 0.3616  data_time: 0.0066  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:32:39 d2.utils.events]: \u001b[0m eta: 0:01:36  iter: 2739  total_loss: 0.07681  loss_cls: 0.01852  loss_box_reg: 0.04321  loss_rpn_cls: 0.001456  loss_rpn_loc: 0.004892  time: 0.3616  data_time: 0.0062  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:32:46 d2.utils.events]: \u001b[0m eta: 0:01:28  iter: 2759  total_loss: 0.06405  loss_cls: 0.01581  loss_box_reg: 0.03799  loss_rpn_cls: 0.001106  loss_rpn_loc: 0.005106  time: 0.3616  data_time: 0.0064  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:32:54 d2.utils.events]: \u001b[0m eta: 0:01:21  iter: 2779  total_loss: 0.06182  loss_cls: 0.01585  loss_box_reg: 0.03718  loss_rpn_cls: 0.0008628  loss_rpn_loc: 0.005069  time: 0.3617  data_time: 0.0066  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:33:01 d2.utils.events]: \u001b[0m eta: 0:01:13  iter: 2799  total_loss: 0.09493  loss_cls: 0.02483  loss_box_reg: 0.05748  loss_rpn_cls: 0.002177  loss_rpn_loc: 0.007531  time: 0.3617  data_time: 0.0064  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:33:08 d2.utils.events]: \u001b[0m eta: 0:01:06  iter: 2819  total_loss: 0.08244  loss_cls: 0.0255  loss_box_reg: 0.04893  loss_rpn_cls: 0.001452  loss_rpn_loc: 0.005081  time: 0.3617  data_time: 0.0064  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:33:16 d2.utils.events]: \u001b[0m eta: 0:00:59  iter: 2839  total_loss: 0.07612  loss_cls: 0.02492  loss_box_reg: 0.04475  loss_rpn_cls: 0.001766  loss_rpn_loc: 0.00981  time: 0.3618  data_time: 0.0065  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:33:23 d2.utils.events]: \u001b[0m eta: 0:00:51  iter: 2859  total_loss: 0.07034  loss_cls: 0.02094  loss_box_reg: 0.03753  loss_rpn_cls: 0.0008602  loss_rpn_loc: 0.005929  time: 0.3618  data_time: 0.0066  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:33:30 d2.utils.events]: \u001b[0m eta: 0:00:44  iter: 2879  total_loss: 0.0814  loss_cls: 0.02  loss_box_reg: 0.05217  loss_rpn_cls: 0.0009609  loss_rpn_loc: 0.004854  time: 0.3618  data_time: 0.0065  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:33:37 d2.utils.events]: \u001b[0m eta: 0:00:36  iter: 2899  total_loss: 0.06298  loss_cls: 0.0171  loss_box_reg: 0.03357  loss_rpn_cls: 0.0008162  loss_rpn_loc: 0.006532  time: 0.3618  data_time: 0.0065  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:33:45 d2.utils.events]: \u001b[0m eta: 0:00:29  iter: 2919  total_loss: 0.0777  loss_cls: 0.021  loss_box_reg: 0.04668  loss_rpn_cls: 0.001267  loss_rpn_loc: 0.006427  time: 0.3619  data_time: 0.0066  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:33:52 d2.utils.events]: \u001b[0m eta: 0:00:22  iter: 2939  total_loss: 0.07788  loss_cls: 0.02142  loss_box_reg: 0.04901  loss_rpn_cls: 0.001438  loss_rpn_loc: 0.004995  time: 0.3619  data_time: 0.0064  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:34:00 d2.utils.events]: \u001b[0m eta: 0:00:14  iter: 2959  total_loss: 0.09211  loss_cls: 0.02272  loss_box_reg: 0.04754  loss_rpn_cls: 0.001748  loss_rpn_loc: 0.008685  time: 0.3620  data_time: 0.0067  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:34:07 d2.utils.events]: \u001b[0m eta: 0:00:07  iter: 2979  total_loss: 0.08806  loss_cls: 0.02605  loss_box_reg: 0.0453  loss_rpn_cls: 0.001476  loss_rpn_loc: 0.005293  time: 0.3620  data_time: 0.0065  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:34:16 d2.utils.events]: \u001b[0m eta: 0:00:00  iter: 2999  total_loss: 0.06421  loss_cls: 0.01951  loss_box_reg: 0.03543  loss_rpn_cls: 0.002237  loss_rpn_loc: 0.005338  time: 0.3621  data_time: 0.0067  lr: 0.0125  max_mem: 4541M\n",
      "\u001b[32m[01/30 21:34:16 d2.engine.hooks]: \u001b[0mOverall training speed: 2998 iterations in 0:18:05 (0.3621 s / it)\n",
      "\u001b[32m[01/30 21:34:16 d2.engine.hooks]: \u001b[0mTotal training time: 0:18:10 (0:00:05 on hooks)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./output/latex-dilation/model_final.pth'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type_ = \"/latex-dilation/\"\n",
    "\n",
    "train_set_name = \"my_dataset_train_\" + type_ + \"1\"\n",
    "\n",
    "# Train set\n",
    "DatasetCatalog.register(train_set_name, get_train_latex_with_dilation)\n",
    "MetadataCatalog.get(train_set_name).set(thing_classes=[\"table\"])\n",
    "text_metadata_train = MetadataCatalog.get(train_set_name)\n",
    "\n",
    "cfg = get_cfg()\n",
    "# \"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"\n",
    "# faster_rcnn_X_101_32x8d_FPN_3x.yaml\n",
    "#cfg.merge_from_file(\"/data/rali5/Tmp/yockelle/TableBank/TableBank/output/word/X152/config_word_X152.yaml\")\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.DATASETS.TRAIN = (train_set_name,)\n",
    "cfg.DATASETS.TEST = ()\n",
    "cfg.DATALOADER.NUM_WORKERS = 4\n",
    "cfg.DATALOADER.FILTER_EMPTY_ANNOTATIONS = False\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")  # Let training initialize from model zoo\n",
    "cfg.SOLVER.IMS_PER_BATCH = 4\n",
    "cfg.SOLVER.BASE_LR = 0.0125  # pick a good LR\n",
    "cfg.SOLVER.MAX_ITER = 3000    # 300 iterations seems good enough for this toy dataset; you will need to train longer for a practical dataset\n",
    "cfg.SOLVER.STEPS = []        # do not decay learning rate\n",
    "#cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, and good enough for this toy dataset (default: 512)\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 256\n",
    "cfg.MODEL.PANOPTIC_FPN.COMBINE.INSTANCES_CONFIDENCE_THRESH = 0.5\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n",
    "cfg.TEST.EVAL_PERIOD = 1000\n",
    "CUDA_LAUNCH_BLOCKING = 1\n",
    "\n",
    "os.makedirs(cfg.OUTPUT_DIR + type_, exist_ok=True)\n",
    "#trainer = DefaultTrainer(cfg) \n",
    "trainer = CocoTrainer(cfg) \n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()\n",
    "\n",
    "shutil.move(cfg.OUTPUT_DIR + \"/\" + \"model_final.pth\", cfg.OUTPUT_DIR + type_ + \"model_final.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[01/31 13:54:14 d2.engine.defaults]: \u001b[0mModel:\n",
      "GeneralizedRCNN(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelMaxPool()\n",
      "    (bottom_up): ResNet(\n",
      "      (stem): BasicStem(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (res2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (4): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (5): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res5): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): StandardROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc_relu1): ReLU()\n",
      "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (fc_relu2): ReLU()\n",
      "    )\n",
      "    (box_predictor): FastRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m[01/31 13:57:06 d2.data.build]: \u001b[0mDistribution of instances among all 1 categories:\n",
      "\u001b[36m|  category  | #instances   |\n",
      "|:----------:|:-------------|\n",
      "|   table    | 712293       |\n",
      "|            |              |\u001b[0m\n",
      "\u001b[32m[01/31 13:57:06 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
      "\u001b[32m[01/31 13:57:06 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
      "\u001b[32m[01/31 13:57:06 d2.data.common]: \u001b[0mSerializing 561597 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[01/31 13:57:13 d2.data.common]: \u001b[0mSerialized dataset takes 248.46 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (2, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (2,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (4, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (4,) in the model! You might want to double check if this is expected.\n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_predictor.cls_score.{bias, weight}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[01/31 13:57:14 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n",
      "\u001b[32m[01/31 13:57:22 d2.utils.events]: \u001b[0m eta: 0:18:39  iter: 19  total_loss: 0.717  loss_cls: 0.3906  loss_box_reg: 0.1575  loss_rpn_cls: 0.05976  loss_rpn_loc: 0.01413  time: 0.3755  data_time: 0.0295  lr: 0.00024976  max_mem: 4535M\n",
      "\u001b[32m[01/31 13:57:30 d2.utils.events]: \u001b[0m eta: 0:19:09  iter: 39  total_loss: 0.4897  loss_cls: 0.191  loss_box_reg: 0.2794  loss_rpn_cls: 0.01584  loss_rpn_loc: 0.007701  time: 0.3853  data_time: 0.0084  lr: 0.00049951  max_mem: 4535M\n",
      "\u001b[32m[01/31 13:57:37 d2.utils.events]: \u001b[0m eta: 0:18:42  iter: 59  total_loss: 0.4488  loss_cls: 0.1405  loss_box_reg: 0.2853  loss_rpn_cls: 0.01302  loss_rpn_loc: 0.007797  time: 0.3815  data_time: 0.0071  lr: 0.00074926  max_mem: 4535M\n",
      "\u001b[32m[01/31 13:57:45 d2.utils.events]: \u001b[0m eta: 0:18:43  iter: 79  total_loss: 0.3464  loss_cls: 0.08985  loss_box_reg: 0.2495  loss_rpn_cls: 0.005448  loss_rpn_loc: 0.008655  time: 0.3819  data_time: 0.0086  lr: 0.00099901  max_mem: 4535M\n",
      "\u001b[32m[01/31 13:57:53 d2.utils.events]: \u001b[0m eta: 0:18:42  iter: 99  total_loss: 0.2221  loss_cls: 0.0687  loss_box_reg: 0.1456  loss_rpn_cls: 0.003162  loss_rpn_loc: 0.009216  time: 0.3830  data_time: 0.0080  lr: 0.0012488  max_mem: 4535M\n",
      "\u001b[32m[01/31 13:58:00 d2.utils.events]: \u001b[0m eta: 0:18:37  iter: 119  total_loss: 0.1941  loss_cls: 0.06427  loss_box_reg: 0.1118  loss_rpn_cls: 0.00343  loss_rpn_loc: 0.008088  time: 0.3836  data_time: 0.0088  lr: 0.0014985  max_mem: 4535M\n",
      "\u001b[32m[01/31 13:58:08 d2.utils.events]: \u001b[0m eta: 0:18:30  iter: 139  total_loss: 0.1601  loss_cls: 0.05662  loss_box_reg: 0.08856  loss_rpn_cls: 0.001626  loss_rpn_loc: 0.009204  time: 0.3845  data_time: 0.0088  lr: 0.0017483  max_mem: 4535M\n",
      "\u001b[32m[01/31 13:58:16 d2.utils.events]: \u001b[0m eta: 0:18:22  iter: 159  total_loss: 0.1547  loss_cls: 0.0624  loss_box_reg: 0.07836  loss_rpn_cls: 0.003015  loss_rpn_loc: 0.009621  time: 0.3848  data_time: 0.0080  lr: 0.001998  max_mem: 4535M\n",
      "\u001b[32m[01/31 13:58:24 d2.utils.events]: \u001b[0m eta: 0:18:14  iter: 179  total_loss: 0.1443  loss_cls: 0.04735  loss_box_reg: 0.07204  loss_rpn_cls: 0.001113  loss_rpn_loc: 0.006822  time: 0.3843  data_time: 0.0083  lr: 0.0022478  max_mem: 4535M\n",
      "\u001b[32m[01/31 13:58:31 d2.utils.events]: \u001b[0m eta: 0:18:07  iter: 199  total_loss: 0.1373  loss_cls: 0.04414  loss_box_reg: 0.06405  loss_rpn_cls: 0.001393  loss_rpn_loc: 0.00915  time: 0.3847  data_time: 0.0085  lr: 0.0024975  max_mem: 4535M\n",
      "\u001b[32m[01/31 13:58:39 d2.utils.events]: \u001b[0m eta: 0:17:59  iter: 219  total_loss: 0.1508  loss_cls: 0.05117  loss_box_reg: 0.07526  loss_rpn_cls: 0.003556  loss_rpn_loc: 0.01082  time: 0.3846  data_time: 0.0069  lr: 0.0027473  max_mem: 4535M\n",
      "\u001b[32m[01/31 13:58:47 d2.utils.events]: \u001b[0m eta: 0:17:50  iter: 239  total_loss: 0.1249  loss_cls: 0.03858  loss_box_reg: 0.06747  loss_rpn_cls: 0.001405  loss_rpn_loc: 0.007899  time: 0.3844  data_time: 0.0085  lr: 0.002997  max_mem: 4535M\n",
      "\u001b[32m[01/31 13:58:54 d2.utils.events]: \u001b[0m eta: 0:17:40  iter: 259  total_loss: 0.1487  loss_cls: 0.04414  loss_box_reg: 0.07545  loss_rpn_cls: 0.0007095  loss_rpn_loc: 0.009041  time: 0.3840  data_time: 0.0065  lr: 0.0032468  max_mem: 4535M\n",
      "\u001b[32m[01/31 13:59:02 d2.utils.events]: \u001b[0m eta: 0:17:30  iter: 279  total_loss: 0.1322  loss_cls: 0.04135  loss_box_reg: 0.07666  loss_rpn_cls: 0.001322  loss_rpn_loc: 0.008527  time: 0.3832  data_time: 0.0070  lr: 0.0034965  max_mem: 4535M\n",
      "\u001b[32m[01/31 13:59:09 d2.utils.events]: \u001b[0m eta: 0:17:23  iter: 299  total_loss: 0.1138  loss_cls: 0.04492  loss_box_reg: 0.06838  loss_rpn_cls: 0.0008074  loss_rpn_loc: 0.004168  time: 0.3832  data_time: 0.0087  lr: 0.0037463  max_mem: 4535M\n",
      "\u001b[32m[01/31 13:59:17 d2.utils.events]: \u001b[0m eta: 0:17:18  iter: 319  total_loss: 0.1451  loss_cls: 0.04366  loss_box_reg: 0.07943  loss_rpn_cls: 0.01515  loss_rpn_loc: 0.008565  time: 0.3839  data_time: 0.0085  lr: 0.003996  max_mem: 4535M\n",
      "\u001b[32m[01/31 13:59:25 d2.utils.events]: \u001b[0m eta: 0:17:11  iter: 339  total_loss: 0.1385  loss_cls: 0.04755  loss_box_reg: 0.07578  loss_rpn_cls: 0.004271  loss_rpn_loc: 0.01197  time: 0.3844  data_time: 0.0096  lr: 0.0042458  max_mem: 4535M\n",
      "\u001b[32m[01/31 13:59:33 d2.utils.events]: \u001b[0m eta: 0:17:04  iter: 359  total_loss: 0.1053  loss_cls: 0.03912  loss_box_reg: 0.05445  loss_rpn_cls: 0.001444  loss_rpn_loc: 0.008559  time: 0.3844  data_time: 0.0083  lr: 0.0044955  max_mem: 4535M\n",
      "\u001b[32m[01/31 13:59:41 d2.utils.events]: \u001b[0m eta: 0:16:56  iter: 379  total_loss: 0.1105  loss_cls: 0.03235  loss_box_reg: 0.06066  loss_rpn_cls: 0.002657  loss_rpn_loc: 0.008601  time: 0.3841  data_time: 0.0084  lr: 0.0047453  max_mem: 4535M\n",
      "\u001b[32m[01/31 13:59:48 d2.utils.events]: \u001b[0m eta: 0:16:47  iter: 399  total_loss: 0.09213  loss_cls: 0.03146  loss_box_reg: 0.05688  loss_rpn_cls: 0.002525  loss_rpn_loc: 0.007562  time: 0.3842  data_time: 0.0088  lr: 0.004995  max_mem: 4535M\n",
      "\u001b[32m[01/31 13:59:56 d2.utils.events]: \u001b[0m eta: 0:16:39  iter: 419  total_loss: 0.1054  loss_cls: 0.03809  loss_box_reg: 0.05815  loss_rpn_cls: 0.001287  loss_rpn_loc: 0.006188  time: 0.3840  data_time: 0.0074  lr: 0.0052448  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:00:04 d2.utils.events]: \u001b[0m eta: 0:16:31  iter: 439  total_loss: 0.1015  loss_cls: 0.03582  loss_box_reg: 0.06383  loss_rpn_cls: 0.002459  loss_rpn_loc: 0.006802  time: 0.3841  data_time: 0.0072  lr: 0.0054945  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:00:11 d2.utils.events]: \u001b[0m eta: 0:16:23  iter: 459  total_loss: 0.1277  loss_cls: 0.03482  loss_box_reg: 0.07478  loss_rpn_cls: 0.002615  loss_rpn_loc: 0.006876  time: 0.3839  data_time: 0.0081  lr: 0.0057443  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:00:19 d2.utils.events]: \u001b[0m eta: 0:16:16  iter: 479  total_loss: 0.1255  loss_cls: 0.03817  loss_box_reg: 0.07157  loss_rpn_cls: 0.002775  loss_rpn_loc: 0.009467  time: 0.3840  data_time: 0.0086  lr: 0.005994  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:00:27 d2.utils.events]: \u001b[0m eta: 0:16:09  iter: 499  total_loss: 0.1206  loss_cls: 0.0392  loss_box_reg: 0.06999  loss_rpn_cls: 0.002795  loss_rpn_loc: 0.007667  time: 0.3843  data_time: 0.0088  lr: 0.0062438  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:00:35 d2.utils.events]: \u001b[0m eta: 0:16:01  iter: 519  total_loss: 0.09609  loss_cls: 0.02718  loss_box_reg: 0.05638  loss_rpn_cls: 0.001031  loss_rpn_loc: 0.007499  time: 0.3846  data_time: 0.0087  lr: 0.0064935  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:00:42 d2.utils.events]: \u001b[0m eta: 0:15:53  iter: 539  total_loss: 0.09607  loss_cls: 0.02281  loss_box_reg: 0.05928  loss_rpn_cls: 0.001502  loss_rpn_loc: 0.004727  time: 0.3845  data_time: 0.0081  lr: 0.0067433  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:00:50 d2.utils.events]: \u001b[0m eta: 0:15:45  iter: 559  total_loss: 0.09141  loss_cls: 0.02751  loss_box_reg: 0.06163  loss_rpn_cls: 0.001026  loss_rpn_loc: 0.007286  time: 0.3843  data_time: 0.0084  lr: 0.006993  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:00:58 d2.utils.events]: \u001b[0m eta: 0:15:38  iter: 579  total_loss: 0.1026  loss_cls: 0.03154  loss_box_reg: 0.05387  loss_rpn_cls: 0.002293  loss_rpn_loc: 0.008589  time: 0.3846  data_time: 0.0087  lr: 0.0072428  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:01:05 d2.utils.events]: \u001b[0m eta: 0:15:30  iter: 599  total_loss: 0.08784  loss_cls: 0.02164  loss_box_reg: 0.05299  loss_rpn_cls: 0.001202  loss_rpn_loc: 0.009929  time: 0.3843  data_time: 0.0071  lr: 0.0074925  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:01:13 d2.utils.events]: \u001b[0m eta: 0:15:22  iter: 619  total_loss: 0.114  loss_cls: 0.03335  loss_box_reg: 0.06394  loss_rpn_cls: 0.002102  loss_rpn_loc: 0.007389  time: 0.3843  data_time: 0.0085  lr: 0.0077423  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:01:21 d2.utils.events]: \u001b[0m eta: 0:15:15  iter: 639  total_loss: 0.117  loss_cls: 0.03454  loss_box_reg: 0.07013  loss_rpn_cls: 0.001904  loss_rpn_loc: 0.008826  time: 0.3844  data_time: 0.0083  lr: 0.007992  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:01:29 d2.utils.events]: \u001b[0m eta: 0:15:07  iter: 659  total_loss: 0.09948  loss_cls: 0.02644  loss_box_reg: 0.06218  loss_rpn_cls: 0.001037  loss_rpn_loc: 0.005892  time: 0.3843  data_time: 0.0088  lr: 0.0082418  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:01:36 d2.utils.events]: \u001b[0m eta: 0:14:59  iter: 679  total_loss: 0.1133  loss_cls: 0.03279  loss_box_reg: 0.06793  loss_rpn_cls: 0.001247  loss_rpn_loc: 0.005857  time: 0.3844  data_time: 0.0087  lr: 0.0084915  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:01:44 d2.utils.events]: \u001b[0m eta: 0:14:51  iter: 699  total_loss: 0.0964  loss_cls: 0.02917  loss_box_reg: 0.05732  loss_rpn_cls: 0.001061  loss_rpn_loc: 0.006503  time: 0.3843  data_time: 0.0082  lr: 0.0087413  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:01:52 d2.utils.events]: \u001b[0m eta: 0:14:43  iter: 719  total_loss: 0.09977  loss_cls: 0.0231  loss_box_reg: 0.05324  loss_rpn_cls: 0.001099  loss_rpn_loc: 0.01045  time: 0.3841  data_time: 0.0086  lr: 0.008991  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:01:59 d2.utils.events]: \u001b[0m eta: 0:14:35  iter: 739  total_loss: 0.08012  loss_cls: 0.02142  loss_box_reg: 0.04706  loss_rpn_cls: 0.001522  loss_rpn_loc: 0.00744  time: 0.3841  data_time: 0.0079  lr: 0.0092408  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:02:07 d2.utils.events]: \u001b[0m eta: 0:14:27  iter: 759  total_loss: 0.1043  loss_cls: 0.03219  loss_box_reg: 0.06136  loss_rpn_cls: 0.0009692  loss_rpn_loc: 0.01067  time: 0.3840  data_time: 0.0078  lr: 0.0094905  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:02:15 d2.utils.events]: \u001b[0m eta: 0:14:19  iter: 779  total_loss: 0.1004  loss_cls: 0.02556  loss_box_reg: 0.05584  loss_rpn_cls: 0.0007469  loss_rpn_loc: 0.008343  time: 0.3840  data_time: 0.0079  lr: 0.0097403  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:02:22 d2.utils.events]: \u001b[0m eta: 0:14:11  iter: 799  total_loss: 0.103  loss_cls: 0.02609  loss_box_reg: 0.06824  loss_rpn_cls: 0.001829  loss_rpn_loc: 0.00641  time: 0.3839  data_time: 0.0081  lr: 0.00999  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:02:30 d2.utils.events]: \u001b[0m eta: 0:14:03  iter: 819  total_loss: 0.08522  loss_cls: 0.0241  loss_box_reg: 0.05499  loss_rpn_cls: 0.001324  loss_rpn_loc: 0.005225  time: 0.3839  data_time: 0.0085  lr: 0.01024  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:02:38 d2.utils.events]: \u001b[0m eta: 0:13:55  iter: 839  total_loss: 0.08189  loss_cls: 0.02096  loss_box_reg: 0.05359  loss_rpn_cls: 0.0007918  loss_rpn_loc: 0.006463  time: 0.3838  data_time: 0.0086  lr: 0.01049  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:02:45 d2.utils.events]: \u001b[0m eta: 0:13:48  iter: 859  total_loss: 0.1422  loss_cls: 0.03251  loss_box_reg: 0.0936  loss_rpn_cls: 0.0006524  loss_rpn_loc: 0.008562  time: 0.3839  data_time: 0.0085  lr: 0.010739  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:02:53 d2.utils.events]: \u001b[0m eta: 0:13:39  iter: 879  total_loss: 0.0971  loss_cls: 0.02516  loss_box_reg: 0.06112  loss_rpn_cls: 0.001443  loss_rpn_loc: 0.005379  time: 0.3839  data_time: 0.0085  lr: 0.010989  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:03:01 d2.utils.events]: \u001b[0m eta: 0:13:32  iter: 899  total_loss: 0.1118  loss_cls: 0.02795  loss_box_reg: 0.06699  loss_rpn_cls: 0.001764  loss_rpn_loc: 0.007951  time: 0.3840  data_time: 0.0085  lr: 0.011239  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:03:08 d2.utils.events]: \u001b[0m eta: 0:13:24  iter: 919  total_loss: 0.09255  loss_cls: 0.02446  loss_box_reg: 0.05599  loss_rpn_cls: 0.001459  loss_rpn_loc: 0.007877  time: 0.3840  data_time: 0.0068  lr: 0.011489  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:03:16 d2.utils.events]: \u001b[0m eta: 0:13:16  iter: 939  total_loss: 0.1134  loss_cls: 0.03687  loss_box_reg: 0.06161  loss_rpn_cls: 0.001163  loss_rpn_loc: 0.006946  time: 0.3837  data_time: 0.0082  lr: 0.011738  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:03:24 d2.utils.events]: \u001b[0m eta: 0:13:08  iter: 959  total_loss: 0.104  loss_cls: 0.02721  loss_box_reg: 0.07031  loss_rpn_cls: 0.001452  loss_rpn_loc: 0.008432  time: 0.3837  data_time: 0.0082  lr: 0.011988  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:03:31 d2.utils.events]: \u001b[0m eta: 0:13:00  iter: 979  total_loss: 0.1068  loss_cls: 0.02984  loss_box_reg: 0.06196  loss_rpn_cls: 0.001621  loss_rpn_loc: 0.006752  time: 0.3835  data_time: 0.0077  lr: 0.012238  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:03:39 d2.utils.events]: \u001b[0m eta: 0:12:52  iter: 999  total_loss: 0.1201  loss_cls: 0.03071  loss_box_reg: 0.07443  loss_rpn_cls: 0.002048  loss_rpn_loc: 0.008851  time: 0.3834  data_time: 0.0077  lr: 0.012488  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:03:46 d2.utils.events]: \u001b[0m eta: 0:12:44  iter: 1019  total_loss: 0.09914  loss_cls: 0.02324  loss_box_reg: 0.05541  loss_rpn_cls: 0.001425  loss_rpn_loc: 0.009227  time: 0.3835  data_time: 0.0082  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:03:54 d2.utils.events]: \u001b[0m eta: 0:12:36  iter: 1039  total_loss: 0.09733  loss_cls: 0.02356  loss_box_reg: 0.05851  loss_rpn_cls: 0.001152  loss_rpn_loc: 0.006703  time: 0.3834  data_time: 0.0077  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:04:02 d2.utils.events]: \u001b[0m eta: 0:12:28  iter: 1059  total_loss: 0.07739  loss_cls: 0.01901  loss_box_reg: 0.04817  loss_rpn_cls: 0.001704  loss_rpn_loc: 0.007183  time: 0.3834  data_time: 0.0085  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:04:09 d2.utils.events]: \u001b[0m eta: 0:12:20  iter: 1079  total_loss: 0.08574  loss_cls: 0.02498  loss_box_reg: 0.0475  loss_rpn_cls: 0.001051  loss_rpn_loc: 0.005631  time: 0.3832  data_time: 0.0081  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:04:17 d2.utils.events]: \u001b[0m eta: 0:12:12  iter: 1099  total_loss: 0.08947  loss_cls: 0.02265  loss_box_reg: 0.05654  loss_rpn_cls: 0.0009741  loss_rpn_loc: 0.008886  time: 0.3831  data_time: 0.0081  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:04:25 d2.utils.events]: \u001b[0m eta: 0:12:04  iter: 1119  total_loss: 0.1112  loss_cls: 0.02705  loss_box_reg: 0.06744  loss_rpn_cls: 0.0008331  loss_rpn_loc: 0.009471  time: 0.3832  data_time: 0.0077  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:04:32 d2.utils.events]: \u001b[0m eta: 0:11:56  iter: 1139  total_loss: 0.1092  loss_cls: 0.03291  loss_box_reg: 0.06429  loss_rpn_cls: 0.001603  loss_rpn_loc: 0.006046  time: 0.3831  data_time: 0.0071  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:04:40 d2.utils.events]: \u001b[0m eta: 0:11:49  iter: 1159  total_loss: 0.1205  loss_cls: 0.0404  loss_box_reg: 0.06281  loss_rpn_cls: 0.003913  loss_rpn_loc: 0.007124  time: 0.3834  data_time: 0.0086  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:04:48 d2.utils.events]: \u001b[0m eta: 0:11:41  iter: 1179  total_loss: 0.1116  loss_cls: 0.03128  loss_box_reg: 0.06171  loss_rpn_cls: 0.003899  loss_rpn_loc: 0.008685  time: 0.3833  data_time: 0.0075  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:04:56 d2.utils.events]: \u001b[0m eta: 0:11:33  iter: 1199  total_loss: 0.1183  loss_cls: 0.03821  loss_box_reg: 0.05908  loss_rpn_cls: 0.004413  loss_rpn_loc: 0.008817  time: 0.3833  data_time: 0.0067  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:05:04 d2.utils.events]: \u001b[0m eta: 0:11:26  iter: 1219  total_loss: 0.09658  loss_cls: 0.02751  loss_box_reg: 0.05252  loss_rpn_cls: 0.003676  loss_rpn_loc: 0.01104  time: 0.3835  data_time: 0.0084  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:05:11 d2.utils.events]: \u001b[0m eta: 0:11:18  iter: 1239  total_loss: 0.08804  loss_cls: 0.0209  loss_box_reg: 0.05429  loss_rpn_cls: 0.002432  loss_rpn_loc: 0.006451  time: 0.3833  data_time: 0.0072  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:05:19 d2.utils.events]: \u001b[0m eta: 0:11:10  iter: 1259  total_loss: 0.08355  loss_cls: 0.01997  loss_box_reg: 0.05245  loss_rpn_cls: 0.002369  loss_rpn_loc: 0.009501  time: 0.3833  data_time: 0.0081  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:05:26 d2.utils.events]: \u001b[0m eta: 0:11:03  iter: 1279  total_loss: 0.09169  loss_cls: 0.02622  loss_box_reg: 0.04794  loss_rpn_cls: 0.00423  loss_rpn_loc: 0.01101  time: 0.3832  data_time: 0.0067  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:05:34 d2.utils.events]: \u001b[0m eta: 0:10:55  iter: 1299  total_loss: 0.1124  loss_cls: 0.03255  loss_box_reg: 0.05508  loss_rpn_cls: 0.004953  loss_rpn_loc: 0.006436  time: 0.3832  data_time: 0.0069  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:05:41 d2.utils.events]: \u001b[0m eta: 0:10:47  iter: 1319  total_loss: 0.08817  loss_cls: 0.02594  loss_box_reg: 0.05213  loss_rpn_cls: 0.003658  loss_rpn_loc: 0.009198  time: 0.3831  data_time: 0.0071  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:05:49 d2.utils.events]: \u001b[0m eta: 0:10:39  iter: 1339  total_loss: 0.09947  loss_cls: 0.02596  loss_box_reg: 0.05834  loss_rpn_cls: 0.002174  loss_rpn_loc: 0.009448  time: 0.3832  data_time: 0.0086  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:05:57 d2.utils.events]: \u001b[0m eta: 0:10:31  iter: 1359  total_loss: 0.09229  loss_cls: 0.02745  loss_box_reg: 0.05592  loss_rpn_cls: 0.002456  loss_rpn_loc: 0.009226  time: 0.3831  data_time: 0.0074  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:06:05 d2.utils.events]: \u001b[0m eta: 0:10:24  iter: 1379  total_loss: 0.116  loss_cls: 0.03328  loss_box_reg: 0.06996  loss_rpn_cls: 0.001793  loss_rpn_loc: 0.007167  time: 0.3831  data_time: 0.0072  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:06:12 d2.utils.events]: \u001b[0m eta: 0:10:16  iter: 1399  total_loss: 0.08836  loss_cls: 0.02304  loss_box_reg: 0.05035  loss_rpn_cls: 0.002219  loss_rpn_loc: 0.007546  time: 0.3832  data_time: 0.0084  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:06:20 d2.utils.events]: \u001b[0m eta: 0:10:08  iter: 1419  total_loss: 0.08405  loss_cls: 0.02411  loss_box_reg: 0.05108  loss_rpn_cls: 0.001611  loss_rpn_loc: 0.007991  time: 0.3831  data_time: 0.0076  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:06:27 d2.utils.events]: \u001b[0m eta: 0:10:00  iter: 1439  total_loss: 0.0844  loss_cls: 0.02299  loss_box_reg: 0.04729  loss_rpn_cls: 0.00213  loss_rpn_loc: 0.007301  time: 0.3830  data_time: 0.0073  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:06:35 d2.utils.events]: \u001b[0m eta: 0:09:52  iter: 1459  total_loss: 0.0883  loss_cls: 0.02023  loss_box_reg: 0.05103  loss_rpn_cls: 0.001212  loss_rpn_loc: 0.00786  time: 0.3830  data_time: 0.0072  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:06:43 d2.utils.events]: \u001b[0m eta: 0:09:45  iter: 1479  total_loss: 0.08002  loss_cls: 0.02315  loss_box_reg: 0.04967  loss_rpn_cls: 0.0009058  loss_rpn_loc: 0.006881  time: 0.3831  data_time: 0.0087  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:06:51 d2.utils.events]: \u001b[0m eta: 0:09:37  iter: 1499  total_loss: 0.1066  loss_cls: 0.02697  loss_box_reg: 0.06577  loss_rpn_cls: 0.001725  loss_rpn_loc: 0.006006  time: 0.3831  data_time: 0.0085  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:06:58 d2.utils.events]: \u001b[0m eta: 0:09:29  iter: 1519  total_loss: 0.08789  loss_cls: 0.02393  loss_box_reg: 0.05393  loss_rpn_cls: 0.001453  loss_rpn_loc: 0.006082  time: 0.3830  data_time: 0.0072  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:07:06 d2.utils.events]: \u001b[0m eta: 0:09:21  iter: 1539  total_loss: 0.08434  loss_cls: 0.0189  loss_box_reg: 0.05449  loss_rpn_cls: 0.001126  loss_rpn_loc: 0.007767  time: 0.3830  data_time: 0.0076  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:07:13 d2.utils.events]: \u001b[0m eta: 0:09:13  iter: 1559  total_loss: 0.07291  loss_cls: 0.01781  loss_box_reg: 0.04727  loss_rpn_cls: 0.0009786  loss_rpn_loc: 0.005699  time: 0.3830  data_time: 0.0081  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:07:21 d2.utils.events]: \u001b[0m eta: 0:09:06  iter: 1579  total_loss: 0.11  loss_cls: 0.02525  loss_box_reg: 0.06452  loss_rpn_cls: 0.001328  loss_rpn_loc: 0.006399  time: 0.3830  data_time: 0.0080  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:07:29 d2.utils.events]: \u001b[0m eta: 0:08:58  iter: 1599  total_loss: 0.08403  loss_cls: 0.01882  loss_box_reg: 0.04767  loss_rpn_cls: 0.005047  loss_rpn_loc: 0.005812  time: 0.3830  data_time: 0.0074  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:07:37 d2.utils.events]: \u001b[0m eta: 0:08:50  iter: 1619  total_loss: 0.08786  loss_cls: 0.02283  loss_box_reg: 0.05014  loss_rpn_cls: 0.001831  loss_rpn_loc: 0.006531  time: 0.3830  data_time: 0.0071  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:07:44 d2.utils.events]: \u001b[0m eta: 0:08:43  iter: 1639  total_loss: 0.07921  loss_cls: 0.01946  loss_box_reg: 0.04834  loss_rpn_cls: 0.0009633  loss_rpn_loc: 0.00606  time: 0.3830  data_time: 0.0082  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:07:52 d2.utils.events]: \u001b[0m eta: 0:08:35  iter: 1659  total_loss: 0.07058  loss_cls: 0.01679  loss_box_reg: 0.04451  loss_rpn_cls: 0.001166  loss_rpn_loc: 0.007739  time: 0.3832  data_time: 0.0090  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:08:00 d2.utils.events]: \u001b[0m eta: 0:08:27  iter: 1679  total_loss: 0.07661  loss_cls: 0.02026  loss_box_reg: 0.04283  loss_rpn_cls: 0.0008671  loss_rpn_loc: 0.009843  time: 0.3832  data_time: 0.0083  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:08:08 d2.utils.events]: \u001b[0m eta: 0:08:20  iter: 1699  total_loss: 0.07722  loss_cls: 0.02441  loss_box_reg: 0.04592  loss_rpn_cls: 0.00122  loss_rpn_loc: 0.005555  time: 0.3832  data_time: 0.0070  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:08:15 d2.utils.events]: \u001b[0m eta: 0:08:13  iter: 1719  total_loss: 0.08408  loss_cls: 0.02238  loss_box_reg: 0.04954  loss_rpn_cls: 0.001095  loss_rpn_loc: 0.008714  time: 0.3833  data_time: 0.0082  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:08:23 d2.utils.events]: \u001b[0m eta: 0:08:05  iter: 1739  total_loss: 0.07355  loss_cls: 0.02028  loss_box_reg: 0.04806  loss_rpn_cls: 0.001432  loss_rpn_loc: 0.005825  time: 0.3832  data_time: 0.0074  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:08:31 d2.utils.events]: \u001b[0m eta: 0:07:57  iter: 1759  total_loss: 0.07104  loss_cls: 0.01828  loss_box_reg: 0.04294  loss_rpn_cls: 0.001301  loss_rpn_loc: 0.006396  time: 0.3832  data_time: 0.0081  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:08:38 d2.utils.events]: \u001b[0m eta: 0:07:49  iter: 1779  total_loss: 0.07829  loss_cls: 0.01865  loss_box_reg: 0.04629  loss_rpn_cls: 0.001568  loss_rpn_loc: 0.007877  time: 0.3831  data_time: 0.0082  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:08:46 d2.utils.events]: \u001b[0m eta: 0:07:41  iter: 1799  total_loss: 0.07197  loss_cls: 0.01817  loss_box_reg: 0.04531  loss_rpn_cls: 0.001927  loss_rpn_loc: 0.008071  time: 0.3830  data_time: 0.0064  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:08:53 d2.utils.events]: \u001b[0m eta: 0:07:33  iter: 1819  total_loss: 0.08417  loss_cls: 0.01913  loss_box_reg: 0.04654  loss_rpn_cls: 0.001235  loss_rpn_loc: 0.006294  time: 0.3830  data_time: 0.0072  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:09:01 d2.utils.events]: \u001b[0m eta: 0:07:26  iter: 1839  total_loss: 0.07697  loss_cls: 0.0154  loss_box_reg: 0.04284  loss_rpn_cls: 0.0009961  loss_rpn_loc: 0.005055  time: 0.3830  data_time: 0.0081  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:09:08 d2.utils.events]: \u001b[0m eta: 0:07:18  iter: 1859  total_loss: 0.08017  loss_cls: 0.01769  loss_box_reg: 0.05218  loss_rpn_cls: 0.001052  loss_rpn_loc: 0.006269  time: 0.3829  data_time: 0.0070  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:09:16 d2.utils.events]: \u001b[0m eta: 0:07:10  iter: 1879  total_loss: 0.08497  loss_cls: 0.02053  loss_box_reg: 0.0517  loss_rpn_cls: 0.001363  loss_rpn_loc: 0.006231  time: 0.3828  data_time: 0.0073  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:09:24 d2.utils.events]: \u001b[0m eta: 0:07:02  iter: 1899  total_loss: 0.08904  loss_cls: 0.02139  loss_box_reg: 0.05734  loss_rpn_cls: 0.00159  loss_rpn_loc: 0.008571  time: 0.3827  data_time: 0.0086  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:09:31 d2.utils.events]: \u001b[0m eta: 0:06:55  iter: 1919  total_loss: 0.07707  loss_cls: 0.02311  loss_box_reg: 0.03423  loss_rpn_cls: 0.001497  loss_rpn_loc: 0.006621  time: 0.3828  data_time: 0.0072  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:09:39 d2.utils.events]: \u001b[0m eta: 0:06:47  iter: 1939  total_loss: 0.07108  loss_cls: 0.02037  loss_box_reg: 0.0438  loss_rpn_cls: 0.000917  loss_rpn_loc: 0.004514  time: 0.3829  data_time: 0.0073  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:09:47 d2.utils.events]: \u001b[0m eta: 0:06:40  iter: 1959  total_loss: 0.07614  loss_cls: 0.02166  loss_box_reg: 0.04508  loss_rpn_cls: 0.001211  loss_rpn_loc: 0.006223  time: 0.3828  data_time: 0.0078  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:09:54 d2.utils.events]: \u001b[0m eta: 0:06:32  iter: 1979  total_loss: 0.08272  loss_cls: 0.02111  loss_box_reg: 0.04795  loss_rpn_cls: 0.0018  loss_rpn_loc: 0.006941  time: 0.3829  data_time: 0.0086  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:10:02 d2.utils.events]: \u001b[0m eta: 0:06:25  iter: 1999  total_loss: 0.0678  loss_cls: 0.01749  loss_box_reg: 0.0388  loss_rpn_cls: 0.000955  loss_rpn_loc: 0.008593  time: 0.3829  data_time: 0.0076  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:10:10 d2.utils.events]: \u001b[0m eta: 0:06:17  iter: 2019  total_loss: 0.07312  loss_cls: 0.01756  loss_box_reg: 0.04814  loss_rpn_cls: 0.001422  loss_rpn_loc: 0.005265  time: 0.3829  data_time: 0.0084  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:10:18 d2.utils.events]: \u001b[0m eta: 0:06:10  iter: 2039  total_loss: 0.08697  loss_cls: 0.02066  loss_box_reg: 0.0568  loss_rpn_cls: 0.001  loss_rpn_loc: 0.008243  time: 0.3829  data_time: 0.0077  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:10:25 d2.utils.events]: \u001b[0m eta: 0:06:02  iter: 2059  total_loss: 0.07657  loss_cls: 0.01755  loss_box_reg: 0.04304  loss_rpn_cls: 0.0009285  loss_rpn_loc: 0.006395  time: 0.3829  data_time: 0.0067  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:10:33 d2.utils.events]: \u001b[0m eta: 0:05:55  iter: 2079  total_loss: 0.08893  loss_cls: 0.01641  loss_box_reg: 0.05102  loss_rpn_cls: 0.0009788  loss_rpn_loc: 0.006798  time: 0.3829  data_time: 0.0079  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:10:41 d2.utils.events]: \u001b[0m eta: 0:05:47  iter: 2099  total_loss: 0.07753  loss_cls: 0.02612  loss_box_reg: 0.04529  loss_rpn_cls: 0.0006818  loss_rpn_loc: 0.00506  time: 0.3829  data_time: 0.0076  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:10:48 d2.utils.events]: \u001b[0m eta: 0:05:39  iter: 2119  total_loss: 0.08696  loss_cls: 0.02738  loss_box_reg: 0.04399  loss_rpn_cls: 0.001694  loss_rpn_loc: 0.007566  time: 0.3830  data_time: 0.0085  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:10:56 d2.utils.events]: \u001b[0m eta: 0:05:32  iter: 2139  total_loss: 0.08091  loss_cls: 0.02049  loss_box_reg: 0.04955  loss_rpn_cls: 0.003031  loss_rpn_loc: 0.005794  time: 0.3830  data_time: 0.0077  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:11:04 d2.utils.events]: \u001b[0m eta: 0:05:24  iter: 2159  total_loss: 0.06979  loss_cls: 0.02138  loss_box_reg: 0.04302  loss_rpn_cls: 0.001421  loss_rpn_loc: 0.007631  time: 0.3831  data_time: 0.0083  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:11:12 d2.utils.events]: \u001b[0m eta: 0:05:16  iter: 2179  total_loss: 0.08536  loss_cls: 0.02116  loss_box_reg: 0.04543  loss_rpn_cls: 0.002094  loss_rpn_loc: 0.008252  time: 0.3831  data_time: 0.0089  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:11:20 d2.utils.events]: \u001b[0m eta: 0:05:08  iter: 2199  total_loss: 0.07304  loss_cls: 0.02056  loss_box_reg: 0.03864  loss_rpn_cls: 0.001188  loss_rpn_loc: 0.006788  time: 0.3832  data_time: 0.0079  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:11:27 d2.utils.events]: \u001b[0m eta: 0:05:01  iter: 2219  total_loss: 0.0705  loss_cls: 0.01799  loss_box_reg: 0.04413  loss_rpn_cls: 0.001151  loss_rpn_loc: 0.005855  time: 0.3832  data_time: 0.0083  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:11:35 d2.utils.events]: \u001b[0m eta: 0:04:53  iter: 2239  total_loss: 0.07842  loss_cls: 0.02033  loss_box_reg: 0.04789  loss_rpn_cls: 0.001971  loss_rpn_loc: 0.006438  time: 0.3833  data_time: 0.0084  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:11:43 d2.utils.events]: \u001b[0m eta: 0:04:46  iter: 2259  total_loss: 0.08127  loss_cls: 0.02005  loss_box_reg: 0.04694  loss_rpn_cls: 0.001384  loss_rpn_loc: 0.008726  time: 0.3833  data_time: 0.0090  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:11:51 d2.utils.events]: \u001b[0m eta: 0:04:38  iter: 2279  total_loss: 0.08071  loss_cls: 0.02097  loss_box_reg: 0.04646  loss_rpn_cls: 0.001546  loss_rpn_loc: 0.006073  time: 0.3834  data_time: 0.0085  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:11:58 d2.utils.events]: \u001b[0m eta: 0:04:30  iter: 2299  total_loss: 0.09819  loss_cls: 0.02384  loss_box_reg: 0.05923  loss_rpn_cls: 0.0008673  loss_rpn_loc: 0.005895  time: 0.3833  data_time: 0.0085  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:12:06 d2.utils.events]: \u001b[0m eta: 0:04:22  iter: 2319  total_loss: 0.08258  loss_cls: 0.02718  loss_box_reg: 0.04682  loss_rpn_cls: 0.001638  loss_rpn_loc: 0.00476  time: 0.3833  data_time: 0.0080  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:12:14 d2.utils.events]: \u001b[0m eta: 0:04:15  iter: 2339  total_loss: 0.08929  loss_cls: 0.02398  loss_box_reg: 0.0556  loss_rpn_cls: 0.0009757  loss_rpn_loc: 0.007814  time: 0.3833  data_time: 0.0084  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:12:21 d2.utils.events]: \u001b[0m eta: 0:04:07  iter: 2359  total_loss: 0.09045  loss_cls: 0.02191  loss_box_reg: 0.0553  loss_rpn_cls: 0.00123  loss_rpn_loc: 0.008882  time: 0.3833  data_time: 0.0084  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:12:29 d2.utils.events]: \u001b[0m eta: 0:03:59  iter: 2379  total_loss: 0.08195  loss_cls: 0.02197  loss_box_reg: 0.04553  loss_rpn_cls: 0.0008984  loss_rpn_loc: 0.006536  time: 0.3834  data_time: 0.0085  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:12:37 d2.utils.events]: \u001b[0m eta: 0:03:51  iter: 2399  total_loss: 0.0759  loss_cls: 0.02556  loss_box_reg: 0.04248  loss_rpn_cls: 0.001418  loss_rpn_loc: 0.006467  time: 0.3833  data_time: 0.0080  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:12:44 d2.utils.events]: \u001b[0m eta: 0:03:44  iter: 2419  total_loss: 0.06814  loss_cls: 0.01661  loss_box_reg: 0.03981  loss_rpn_cls: 0.0009982  loss_rpn_loc: 0.006231  time: 0.3833  data_time: 0.0068  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:12:52 d2.utils.events]: \u001b[0m eta: 0:03:36  iter: 2439  total_loss: 0.07924  loss_cls: 0.01981  loss_box_reg: 0.05014  loss_rpn_cls: 0.0007711  loss_rpn_loc: 0.006006  time: 0.3833  data_time: 0.0080  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:13:00 d2.utils.events]: \u001b[0m eta: 0:03:28  iter: 2459  total_loss: 0.06485  loss_cls: 0.01389  loss_box_reg: 0.03796  loss_rpn_cls: 0.0006597  loss_rpn_loc: 0.005673  time: 0.3833  data_time: 0.0080  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:13:08 d2.utils.events]: \u001b[0m eta: 0:03:21  iter: 2479  total_loss: 0.08136  loss_cls: 0.02202  loss_box_reg: 0.05482  loss_rpn_cls: 0.0009647  loss_rpn_loc: 0.008278  time: 0.3833  data_time: 0.0077  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:13:15 d2.utils.events]: \u001b[0m eta: 0:03:13  iter: 2499  total_loss: 0.1326  loss_cls: 0.02623  loss_box_reg: 0.07745  loss_rpn_cls: 0.001384  loss_rpn_loc: 0.006831  time: 0.3833  data_time: 0.0080  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:13:23 d2.utils.events]: \u001b[0m eta: 0:03:05  iter: 2519  total_loss: 0.08112  loss_cls: 0.01901  loss_box_reg: 0.0521  loss_rpn_cls: 0.001795  loss_rpn_loc: 0.008618  time: 0.3833  data_time: 0.0073  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:13:31 d2.utils.events]: \u001b[0m eta: 0:02:57  iter: 2539  total_loss: 0.07345  loss_cls: 0.01445  loss_box_reg: 0.0504  loss_rpn_cls: 0.003663  loss_rpn_loc: 0.00517  time: 0.3833  data_time: 0.0063  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:13:39 d2.utils.events]: \u001b[0m eta: 0:02:50  iter: 2559  total_loss: 0.06921  loss_cls: 0.01785  loss_box_reg: 0.0454  loss_rpn_cls: 0.00173  loss_rpn_loc: 0.004386  time: 0.3834  data_time: 0.0079  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:13:46 d2.utils.events]: \u001b[0m eta: 0:02:42  iter: 2579  total_loss: 0.08914  loss_cls: 0.02314  loss_box_reg: 0.05304  loss_rpn_cls: 0.001221  loss_rpn_loc: 0.007122  time: 0.3834  data_time: 0.0083  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:13:54 d2.utils.events]: \u001b[0m eta: 0:02:34  iter: 2599  total_loss: 0.0678  loss_cls: 0.02034  loss_box_reg: 0.04461  loss_rpn_cls: 0.0008615  loss_rpn_loc: 0.005554  time: 0.3834  data_time: 0.0070  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:14:02 d2.utils.events]: \u001b[0m eta: 0:02:26  iter: 2619  total_loss: 0.06242  loss_cls: 0.01235  loss_box_reg: 0.04031  loss_rpn_cls: 0.0009499  loss_rpn_loc: 0.004436  time: 0.3834  data_time: 0.0078  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:14:09 d2.utils.events]: \u001b[0m eta: 0:02:19  iter: 2639  total_loss: 0.06618  loss_cls: 0.01766  loss_box_reg: 0.03915  loss_rpn_cls: 0.0008295  loss_rpn_loc: 0.003582  time: 0.3834  data_time: 0.0083  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:14:17 d2.utils.events]: \u001b[0m eta: 0:02:11  iter: 2659  total_loss: 0.08182  loss_cls: 0.03085  loss_box_reg: 0.04237  loss_rpn_cls: 0.0009149  loss_rpn_loc: 0.005733  time: 0.3834  data_time: 0.0081  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:14:25 d2.utils.events]: \u001b[0m eta: 0:02:03  iter: 2679  total_loss: 0.09176  loss_cls: 0.02952  loss_box_reg: 0.04992  loss_rpn_cls: 0.001433  loss_rpn_loc: 0.005519  time: 0.3835  data_time: 0.0084  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:14:32 d2.utils.events]: \u001b[0m eta: 0:01:56  iter: 2699  total_loss: 0.09257  loss_cls: 0.02457  loss_box_reg: 0.05227  loss_rpn_cls: 0.004732  loss_rpn_loc: 0.007625  time: 0.3834  data_time: 0.0066  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:14:40 d2.utils.events]: \u001b[0m eta: 0:01:48  iter: 2719  total_loss: 0.07748  loss_cls: 0.01969  loss_box_reg: 0.04415  loss_rpn_cls: 0.001749  loss_rpn_loc: 0.009682  time: 0.3834  data_time: 0.0083  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:14:48 d2.utils.events]: \u001b[0m eta: 0:01:40  iter: 2739  total_loss: 0.08259  loss_cls: 0.02485  loss_box_reg: 0.04574  loss_rpn_cls: 0.001253  loss_rpn_loc: 0.005303  time: 0.3834  data_time: 0.0086  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:14:55 d2.utils.events]: \u001b[0m eta: 0:01:32  iter: 2759  total_loss: 0.07924  loss_cls: 0.02451  loss_box_reg: 0.04434  loss_rpn_cls: 0.001015  loss_rpn_loc: 0.003948  time: 0.3834  data_time: 0.0085  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:15:03 d2.utils.events]: \u001b[0m eta: 0:01:25  iter: 2779  total_loss: 0.08595  loss_cls: 0.02864  loss_box_reg: 0.04416  loss_rpn_cls: 0.001993  loss_rpn_loc: 0.006805  time: 0.3834  data_time: 0.0083  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:15:11 d2.utils.events]: \u001b[0m eta: 0:01:17  iter: 2799  total_loss: 0.07932  loss_cls: 0.02476  loss_box_reg: 0.04808  loss_rpn_cls: 0.001091  loss_rpn_loc: 0.004537  time: 0.3834  data_time: 0.0079  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:15:19 d2.utils.events]: \u001b[0m eta: 0:01:09  iter: 2819  total_loss: 0.07095  loss_cls: 0.01816  loss_box_reg: 0.0428  loss_rpn_cls: 0.001086  loss_rpn_loc: 0.004297  time: 0.3834  data_time: 0.0085  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:15:26 d2.utils.events]: \u001b[0m eta: 0:01:01  iter: 2839  total_loss: 0.07265  loss_cls: 0.01853  loss_box_reg: 0.03773  loss_rpn_cls: 0.002016  loss_rpn_loc: 0.005161  time: 0.3834  data_time: 0.0083  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:15:34 d2.utils.events]: \u001b[0m eta: 0:00:54  iter: 2859  total_loss: 0.07022  loss_cls: 0.02082  loss_box_reg: 0.04128  loss_rpn_cls: 0.001806  loss_rpn_loc: 0.009903  time: 0.3834  data_time: 0.0087  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:15:42 d2.utils.events]: \u001b[0m eta: 0:00:46  iter: 2879  total_loss: 0.07365  loss_cls: 0.02083  loss_box_reg: 0.04766  loss_rpn_cls: 0.0006252  loss_rpn_loc: 0.004341  time: 0.3835  data_time: 0.0086  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:15:50 d2.utils.events]: \u001b[0m eta: 0:00:38  iter: 2899  total_loss: 0.07004  loss_cls: 0.01696  loss_box_reg: 0.03961  loss_rpn_cls: 0.0008906  loss_rpn_loc: 0.005653  time: 0.3835  data_time: 0.0086  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:15:57 d2.utils.events]: \u001b[0m eta: 0:00:31  iter: 2919  total_loss: 0.07973  loss_cls: 0.02016  loss_box_reg: 0.0537  loss_rpn_cls: 0.001279  loss_rpn_loc: 0.004118  time: 0.3835  data_time: 0.0082  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:16:05 d2.utils.events]: \u001b[0m eta: 0:00:23  iter: 2939  total_loss: 0.07331  loss_cls: 0.01997  loss_box_reg: 0.04155  loss_rpn_cls: 0.00102  loss_rpn_loc: 0.005824  time: 0.3835  data_time: 0.0072  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:16:12 d2.utils.events]: \u001b[0m eta: 0:00:15  iter: 2959  total_loss: 0.05583  loss_cls: 0.01174  loss_box_reg: 0.03588  loss_rpn_cls: 0.003484  loss_rpn_loc: 0.00683  time: 0.3834  data_time: 0.0088  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:16:20 d2.utils.events]: \u001b[0m eta: 0:00:07  iter: 2979  total_loss: 0.06257  loss_cls: 0.01519  loss_box_reg: 0.03901  loss_rpn_cls: 0.001296  loss_rpn_loc: 0.00506  time: 0.3833  data_time: 0.0070  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:16:29 d2.utils.events]: \u001b[0m eta: 0:00:00  iter: 2999  total_loss: 0.06122  loss_cls: 0.01739  loss_box_reg: 0.03641  loss_rpn_cls: 0.001293  loss_rpn_loc: 0.004602  time: 0.3834  data_time: 0.0084  lr: 0.0125  max_mem: 4535M\n",
      "\u001b[32m[01/31 14:16:29 d2.engine.hooks]: \u001b[0mOverall training speed: 2998 iterations in 0:19:09 (0.3834 s / it)\n",
      "\u001b[32m[01/31 14:16:29 d2.engine.hooks]: \u001b[0mTotal training time: 0:19:14 (0:00:04 on hooks)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./output/latex-smudge-dilation/model_final.pth'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type_ = \"/latex-smudge-dilation/\"\n",
    "\n",
    "train_set_name = \"my_dataset_train_\" + type_ + \"1\"\n",
    "\n",
    "# Train set\n",
    "DatasetCatalog.register(train_set_name, get_train_latex_with_smuge_and_dilation)\n",
    "MetadataCatalog.get(train_set_name).set(thing_classes=[\"table\"])\n",
    "text_metadata_train = MetadataCatalog.get(train_set_name)\n",
    "\n",
    "cfg = get_cfg()\n",
    "# \"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"\n",
    "# faster_rcnn_X_101_32x8d_FPN_3x.yaml\n",
    "#cfg.merge_from_file(\"/data/rali5/Tmp/yockelle/TableBank/TableBank/output/word/X152/config_word_X152.yaml\")\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
    "cfg.DATASETS.TRAIN = (train_set_name,)\n",
    "cfg.DATASETS.TEST = ()\n",
    "cfg.DATALOADER.NUM_WORKERS = 4\n",
    "cfg.DATALOADER.FILTER_EMPTY_ANNOTATIONS = False\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")  # Let training initialize from model zoo\n",
    "cfg.SOLVER.IMS_PER_BATCH = 4\n",
    "cfg.SOLVER.BASE_LR = 0.0125  # pick a good LR\n",
    "cfg.SOLVER.MAX_ITER = 3000    # 300 iterations seems good enough for this toy dataset; you will need to train longer for a practical dataset\n",
    "cfg.SOLVER.STEPS = []        # do not decay learning rate\n",
    "#cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, and good enough for this toy dataset (default: 512)\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 256\n",
    "cfg.MODEL.PANOPTIC_FPN.COMBINE.INSTANCES_CONFIDENCE_THRESH = 0.5\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n",
    "cfg.TEST.EVAL_PERIOD = 1000\n",
    "CUDA_LAUNCH_BLOCKING = 1\n",
    "\n",
    "os.makedirs(cfg.OUTPUT_DIR + type_, exist_ok=True)\n",
    "#trainer = DefaultTrainer(cfg) \n",
    "trainer = CocoTrainer(cfg) \n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()\n",
    "\n",
    "shutil.move(cfg.OUTPUT_DIR + \"/\" + \"model_final.pth\", cfg.OUTPUT_DIR + type_ + \"model_final.pth\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word + Sumdge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: word-smudge, test: word\n",
      "./output/word-smudge/ model_final.pth\n",
      "TableBank/Detection/images/bedrijfseconomie_h3_6.jpg\n",
      "1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/yockelle/anaconda3/lib/python3.8/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  84.13897200195424\n",
      "Recall:  94.19261636968349\n",
      "F1:  88.8824014173371\n",
      "train: word-smudge, test: latex\n",
      "./output/word-smudge/ model_final.pth\n",
      "TableBank/Detection/images/1507.06442_5.jpg\n",
      "1000\n",
      "Precision:  66.78487051410475\n",
      "Recall:  71.41189592974736\n",
      "F1:  69.02092350724561\n",
      "train: word-smudge, test: publaynet\n",
      "./output/word-smudge/ model_final.pth\n",
      "/data/rali5/Tmp/yockelle/PubLayNet/data/publaynet/val/PMC5134224_00005.jpg\n",
      "1000\n",
      "Precision:  40.28317888136454\n",
      "Recall:  38.6714660176685\n",
      "F1:  39.460872382783116\n",
      "train: word-smudge, test: word-latex\n",
      "./output/word-smudge/ model_final.pth\n",
      "TableBank/Detection/images/bedrijfseconomie_h3_6.jpg\n",
      "2000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-6b86f8b0ce7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train: word-smudge, test: word-latex\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word-smudge\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"word-latex\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"X101\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mthres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtake_the_table_bank_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Precision: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Recall: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/TableBank/get_models_and_set.py\u001b[0m in \u001b[0;36mevaluate_models\u001b[0;34m(train_type_name, test_type_name, size_model, threshold, take_the_table_bank_model, take_only_1k)\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mpredictions_detectron\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"instances\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred_boxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/detectron2/engine/defaults.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, original_image)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"image\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"height\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"width\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/detectron2/modeling/meta_arch/rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, batched_inputs)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \"\"\"\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/detectron2/modeling/meta_arch/rcnn.py\u001b[0m in \u001b[0;36minference\u001b[0;34m(self, batched_inputs, detected_instances, do_postprocess)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdetected_instances\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/detectron2/modeling/backbone/fpn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0;34m\"p2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"p3\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"p6\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \"\"\"\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0mbottom_up_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbottom_up\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mprev_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlateral_convs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbottom_up_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/detectron2/modeling/backbone/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    447\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"stem\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstage_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_out_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                 \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/detectron2/modeling/backbone/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshortcut\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/detectron2/layers/wrappers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    104\u001b[0m                 ), \"SyncBatchNorm does not support empty inputs!\"\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         x = F.conv2d(\n\u001b[0m\u001b[1;32m    107\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"train: word-smudge, test: word\")\n",
    "precision, recall, f1 = evaluate_models(\"word-smudge\", \"word\", \"X101\", threshold=thres, take_the_table_bank_model = False)\n",
    "print(\"Precision: \", precision*100)\n",
    "print(\"Recall: \", recall*100)\n",
    "print(\"F1: \", f1*100)\n",
    "\n",
    "print(\"train: word-smudge, test: latex\")\n",
    "precision, recall, f1 = evaluate_models(\"word-smudge\", \"latex\", \"X101\", threshold=thres, take_the_table_bank_model = False)\n",
    "print(\"Precision: \", precision*100)\n",
    "print(\"Recall: \", recall*100)\n",
    "print(\"F1: \", f1*100)\n",
    "\n",
    "print(\"train: word-smudge, test: publaynet\")\n",
    "precision, recall, f1 = evaluate_models(\"word-smudge\", \"publaynet\", \"X101\", threshold=thres, take_the_table_bank_model = False)\n",
    "print(\"Precision: \", precision*100)\n",
    "print(\"Recall: \", recall*100)\n",
    "print(\"F1: \", f1*100)\n",
    "\n",
    "print(\"train: word-smudge, test: word-latex\")\n",
    "precision, recall, f1 = evaluate_models(\"word-smudge\", \"word-latex\", \"X101\", threshold=thres, take_the_table_bank_model = False)\n",
    "print(\"Precision: \", precision*100)\n",
    "print(\"Recall: \", recall*100)\n",
    "print(\"F1: \", f1*100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word + Dilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: word-dilation, test: word\n",
      "./output/word-dilation/ model_final.pth\n",
      "TableBank/Detection/images/bedrijfseconomie_h3_6.jpg\n",
      "1000\n",
      "Precision:  84.65396786234591\n",
      "Recall:  93.84481728921095\n",
      "F1:  89.01277552229097\n",
      "train: word-dilation, test: latex\n",
      "./output/word-dilation/ model_final.pth\n",
      "TableBank/Detection/images/1507.06442_5.jpg\n",
      "1000\n",
      "Precision:  93.26416827627305\n",
      "Recall:  76.59283333138072\n",
      "F1:  84.110361409471\n",
      "train: word-dilation, test: publaynet\n",
      "./output/word-dilation/ model_final.pth\n",
      "/data/rali5/Tmp/yockelle/PubLayNet/data/publaynet/val/PMC5134224_00005.jpg\n",
      "1000\n",
      "Precision:  90.19386953644918\n",
      "Recall:  61.006953387465266\n",
      "F1:  72.78337628379391\n",
      "train: word-dilation, test: word-latex\n",
      "./output/word-dilation/ model_final.pth\n",
      "TableBank/Detection/images/bedrijfseconomie_h3_6.jpg\n",
      "2000\n",
      "Precision:  86.79916744564115\n",
      "Recall:  88.5079442707258\n",
      "F1:  87.64522784966927\n"
     ]
    }
   ],
   "source": [
    "print(\"train: word-dilation, test: word\")\n",
    "precision, recall, f1 = evaluate_models(\"word-dilation\", \"word\", \"X101\", threshold=thres, take_the_table_bank_model = False)\n",
    "print(\"Precision: \", precision*100)\n",
    "print(\"Recall: \", recall*100)\n",
    "print(\"F1: \", f1*100)\n",
    "\n",
    "print(\"train: word-dilation, test: latex\")\n",
    "precision, recall, f1 = evaluate_models(\"word-dilation\", \"latex\", \"X101\", threshold=thres, take_the_table_bank_model = False)\n",
    "print(\"Precision: \", precision*100)\n",
    "print(\"Recall: \", recall*100)\n",
    "print(\"F1: \", f1*100)\n",
    "\n",
    "print(\"train: word-dilation, test: publaynet\")\n",
    "precision, recall, f1 = evaluate_models(\"word-dilation\", \"publaynet\", \"X101\", threshold=thres, take_the_table_bank_model = False)\n",
    "print(\"Precision: \", precision*100)\n",
    "print(\"Recall: \", recall*100)\n",
    "print(\"F1: \", f1*100)\n",
    "\n",
    "print(\"train: word-dilation, test: word-latex\")\n",
    "precision, recall, f1 = evaluate_models(\"word-dilation\", \"word-latex\", \"X101\", threshold=thres, take_the_table_bank_model = False)\n",
    "print(\"Precision: \", precision*100)\n",
    "print(\"Recall: \", recall*100)\n",
    "print(\"F1: \", f1*100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word + Sumdge + Dilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: word-smudge-dilation, test: word\n",
      "./output/word-smudge-dilation/ model_final.pth\n",
      "TableBank/Detection/images/bedrijfseconomie_h3_6.jpg\n",
      "1000\n",
      "Precision:  85.37386972327185\n",
      "Recall:  95.69038335825314\n",
      "F1:  90.23822409516814\n",
      "train: word-smudge-dilation, test: latex\n",
      "./output/word-smudge-dilation/ model_final.pth\n",
      "TableBank/Detection/images/1507.06442_5.jpg\n",
      "1000\n",
      "Precision:  91.14630941393827\n",
      "Recall:  69.36098310076339\n",
      "F1:  78.77520738041306\n",
      "train: word-smudge-dilation, test: publaynet\n",
      "./output/word-smudge-dilation/ model_final.pth\n",
      "/data/rali5/Tmp/yockelle/PubLayNet/data/publaynet/val/PMC5134224_00005.jpg\n",
      "1000\n",
      "Precision:  88.4893428891804\n",
      "Recall:  44.477567143503556\n",
      "F1:  59.19955105929125\n",
      "train: word-smudge-dilation, test: word-latex\n",
      "./output/word-smudge-dilation/ model_final.pth\n",
      "TableBank/Detection/images/bedrijfseconomie_h3_6.jpg\n",
      "2000\n",
      "Precision:  86.71994362518596\n",
      "Recall:  87.545426455293\n",
      "F1:  87.13072991311788\n"
     ]
    }
   ],
   "source": [
    "print(\"train: word-smudge-dilation, test: word\")\n",
    "precision, recall, f1 = evaluate_models(\"word-smudge-dilation\", \"word\", \"X101\", threshold=thres, take_the_table_bank_model = False)\n",
    "print(\"Precision: \", precision*100)\n",
    "print(\"Recall: \", recall*100)\n",
    "print(\"F1: \", f1*100)\n",
    "\n",
    "print(\"train: word-smudge-dilation, test: latex\")\n",
    "precision, recall, f1 = evaluate_models(\"word-smudge-dilation\", \"latex\", \"X101\", threshold=thres, take_the_table_bank_model = False)\n",
    "print(\"Precision: \", precision*100)\n",
    "print(\"Recall: \", recall*100)\n",
    "print(\"F1: \", f1*100)\n",
    "\n",
    "print(\"train: word-smudge-dilation, test: publaynet\")\n",
    "precision, recall, f1 = evaluate_models(\"word-smudge-dilation\", \"publaynet\", \"X101\", threshold=thres, take_the_table_bank_model = False)\n",
    "print(\"Precision: \", precision*100)\n",
    "print(\"Recall: \", recall*100)\n",
    "print(\"F1: \", f1*100)\n",
    "\n",
    "print(\"train: word-smudge-dilation, test: word-latex\")\n",
    "precision, recall, f1 = evaluate_models(\"word-smudge-dilation\", \"word-latex\", \"X101\", threshold=thres, take_the_table_bank_model = False)\n",
    "print(\"Precision: \", precision*100)\n",
    "print(\"Recall: \", recall*100)\n",
    "print(\"F1: \", f1*100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latex + S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: latex-smudge, test: word\n",
      "./output/latex-smudge/ model_final.pth\n",
      "TableBank/Detection/images/bedrijfseconomie_h3_6.jpg\n",
      "1000\n",
      "Precision:  82.38940732104231\n",
      "Recall:  84.10495381161591\n",
      "F1:  83.23834212957576\n",
      "train: latex-smudge, test: latex\n",
      "./output/latex-smudge/ model_final.pth\n",
      "TableBank/Detection/images/1507.06442_5.jpg\n",
      "1000\n",
      "Precision:  95.62583411902428\n",
      "Recall:  87.60130762618999\n",
      "F1:  91.43785174928058\n",
      "train: latex-smudge, test: publaynet\n",
      "./output/latex-smudge/ model_final.pth\n",
      "/data/rali5/Tmp/yockelle/PubLayNet/data/publaynet/val/PMC5134224_00005.jpg\n",
      "1000\n",
      "Precision:  94.7844227139912\n",
      "Recall:  71.43307805733522\n",
      "F1:  81.46847395645584\n",
      "train: latex-smudge, test: word-latex\n",
      "./output/latex-smudge/ model_final.pth\n",
      "TableBank/Detection/images/bedrijfseconomie_h3_6.jpg\n",
      "2000\n",
      "Precision:  86.18440597442894\n",
      "Recall:  85.18654514520905\n",
      "F1:  85.68257038181774\n"
     ]
    }
   ],
   "source": [
    "print(\"train: latex-smudge, test: word\")\n",
    "precision, recall, f1 = evaluate_models(\"latex-smudge\", \"word\", \"X101\", threshold=thres, take_the_table_bank_model = False)\n",
    "print(\"Precision: \", precision*100)\n",
    "print(\"Recall: \", recall*100)\n",
    "print(\"F1: \", f1*100)\n",
    "\n",
    "print(\"train: latex-smudge, test: latex\")\n",
    "precision, recall, f1 = evaluate_models(\"latex-smudge\", \"latex\", \"X101\", threshold=thres, take_the_table_bank_model = False)\n",
    "print(\"Precision: \", precision*100)\n",
    "print(\"Recall: \", recall*100)\n",
    "print(\"F1: \", f1*100)\n",
    "\n",
    "print(\"train: latex-smudge, test: publaynet\")\n",
    "precision, recall, f1 = evaluate_models(\"latex-smudge\", \"publaynet\", \"X101\", threshold=thres, take_the_table_bank_model = False)\n",
    "print(\"Precision: \", precision*100)\n",
    "print(\"Recall: \", recall*100)\n",
    "print(\"F1: \", f1*100)\n",
    "\n",
    "print(\"train: latex-smudge, test: word-latex\")\n",
    "precision, recall, f1 = evaluate_models(\"latex-smudge\", \"word-latex\", \"X101\", threshold=thres, take_the_table_bank_model = False)\n",
    "print(\"Precision: \", precision*100)\n",
    "print(\"Recall: \", recall*100)\n",
    "print(\"F1: \", f1*100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latex + D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: latex-dilation, test: word\n",
      "./output/latex-dilation/ model_final.pth\n",
      "TableBank/Detection/images/bedrijfseconomie_h3_6.jpg\n",
      "1000\n",
      "Precision:  79.22465015304599\n",
      "Recall:  94.85666797382731\n",
      "F1:  86.33880321877004\n",
      "train: latex-dilation, test: latex\n",
      "./output/latex-dilation/ model_final.pth\n",
      "TableBank/Detection/images/1507.06442_5.jpg\n",
      "1000\n",
      "Precision:  94.03467955120185\n",
      "Recall:  93.81932186713111\n",
      "F1:  93.92687726507761\n",
      "train: latex-dilation, test: publaynet\n",
      "./output/latex-dilation/ model_final.pth\n",
      "/data/rali5/Tmp/yockelle/PubLayNet/data/publaynet/val/PMC5134224_00005.jpg\n",
      "1000\n",
      "Precision:  87.29067916122905\n",
      "Recall:  90.1559120909748\n",
      "F1:  88.70016314526991\n",
      "train: latex-dilation, test: word-latex\n",
      "./output/latex-dilation/ model_final.pth\n",
      "TableBank/Detection/images/bedrijfseconomie_h3_6.jpg\n",
      "2000\n",
      "Precision:  83.24992642323767\n",
      "Recall:  94.53576665249588\n",
      "F1:  88.53463382829266\n"
     ]
    }
   ],
   "source": [
    "print(\"train: latex-dilation, test: word\")\n",
    "precision, recall, f1 = evaluate_models(\"latex-dilation\", \"word\", \"X101\", threshold=thres, take_the_table_bank_model = False)\n",
    "print(\"Precision: \", precision*100)\n",
    "print(\"Recall: \", recall*100)\n",
    "print(\"F1: \", f1*100)\n",
    "\n",
    "print(\"train: latex-dilation, test: latex\")\n",
    "precision, recall, f1 = evaluate_models(\"latex-dilation\", \"latex\", \"X101\", threshold=thres, take_the_table_bank_model = False)\n",
    "print(\"Precision: \", precision*100)\n",
    "print(\"Recall: \", recall*100)\n",
    "print(\"F1: \", f1*100)\n",
    "\n",
    "print(\"train: latex-dilation, test: publaynet\")\n",
    "precision, recall, f1 = evaluate_models(\"latex-dilation\", \"publaynet\", \"X101\", threshold=thres, take_the_table_bank_model = False)\n",
    "print(\"Precision: \", precision*100)\n",
    "print(\"Recall: \", recall*100)\n",
    "print(\"F1: \", f1*100)\n",
    "\n",
    "print(\"train: latex-dilation, test: word-latex\")\n",
    "precision, recall, f1 = evaluate_models(\"latex-dilation\", \"word-latex\", \"X101\", threshold=thres, take_the_table_bank_model = False)\n",
    "print(\"Precision: \", precision*100)\n",
    "print(\"Recall: \", recall*100)\n",
    "print(\"F1: \", f1*100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latex + D + S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: latex-smudge-dilation, test: word\n",
      "./output/latex-smudge-dilation/ model_final.pth\n",
      "TableBank/Detection/images/bedrijfseconomie_h3_6.jpg\n",
      "1000\n",
      "Precision:  77.86773414928292\n",
      "Recall:  95.71440717922424\n",
      "F1:  85.87362680799056\n",
      "train: latex-smudge-dilation, test: latex\n",
      "./output/latex-smudge-dilation/ model_final.pth\n",
      "TableBank/Detection/images/1507.06442_5.jpg\n",
      "1000\n",
      "Precision:  94.66790058946481\n",
      "Recall:  95.15106585891576\n",
      "F1:  94.9088682996599\n",
      "train: latex-smudge-dilation, test: publaynet\n",
      "./output/latex-smudge-dilation/ model_final.pth\n",
      "/data/rali5/Tmp/yockelle/PubLayNet/data/publaynet/val/PMC5134224_00005.jpg\n",
      "1000\n",
      "Precision:  88.74966019485213\n",
      "Recall:  87.17258347703009\n",
      "F1:  87.9540528862677\n",
      "train: latex-smudge-dilation, test: word-latex\n",
      "./output/latex-smudge-dilation/ model_final.pth\n",
      "TableBank/Detection/images/bedrijfseconomie_h3_6.jpg\n",
      "2000\n",
      "Precision:  82.37136424128849\n",
      "Recall:  95.54013846323761\n",
      "F1:  88.46838372321018\n"
     ]
    }
   ],
   "source": [
    "print(\"train: latex-smudge-dilation, test: word\")\n",
    "precision, recall, f1 = evaluate_models(\"latex-smudge-dilation\", \"word\", \"X101\", threshold=thres, take_the_table_bank_model = False)\n",
    "print(\"Precision: \", precision*100)\n",
    "print(\"Recall: \", recall*100)\n",
    "print(\"F1: \", f1*100)\n",
    "\n",
    "print(\"train: latex-smudge-dilation, test: latex\")\n",
    "precision, recall, f1 = evaluate_models(\"latex-smudge-dilation\", \"latex\", \"X101\", threshold=thres, take_the_table_bank_model = False)\n",
    "print(\"Precision: \", precision*100)\n",
    "print(\"Recall: \", recall*100)\n",
    "print(\"F1: \", f1*100)\n",
    "\n",
    "print(\"train: latex-smudge-dilation, test: publaynet\")\n",
    "precision, recall, f1 = evaluate_models(\"latex-smudge-dilation\", \"publaynet\", \"X101\", threshold=thres, take_the_table_bank_model = False)\n",
    "print(\"Precision: \", precision*100)\n",
    "print(\"Recall: \", recall*100)\n",
    "print(\"F1: \", f1*100)\n",
    "\n",
    "print(\"train: latex-smudge-dilation, test: word-latex\")\n",
    "precision, recall, f1 = evaluate_models(\"latex-smudge-dilation\", \"word-latex\", \"X101\", threshold=thres, take_the_table_bank_model = False)\n",
    "print(\"Precision: \", precision*100)\n",
    "print(\"Recall: \", recall*100)\n",
    "print(\"F1: \", f1*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "608c06b448c1de9fbfe61db40b999c2492eeda40b619a01541314540d82445de"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
